{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bi-gru with attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbO_FgGD0MG9"
      },
      "source": [
        "IMPORTING THE LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBH9pFKKPKqu",
        "outputId": "6b16bf32-ba23-4c03-be85-cec9c4f4d670"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "\n",
        "! pip install --user indic\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "import re\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'indic_nlp_library' already exists and is not an empty directory.\n",
            "fatal: destination path 'indic_nlp_resources' already exists and is not an empty directory.\n",
            "Requirement already satisfied: indic in /root/.local/lib/python3.7/site-packages (0.1.2)\n",
            "Requirement already satisfied: pandas>=0.12 in /usr/local/lib/python3.7/dist-packages (from indic) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from indic) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from indic) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.13 in /usr/local/lib/python3.7/dist-packages (from indic) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=1.3 in /usr/local/lib/python3.7/dist-packages (from indic) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.12->indic) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.12->indic) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.13->indic) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.3->indic) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.3->indic) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.3->indic) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.12->indic) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd25xjYU1DVS"
      },
      "source": [
        "HINDI AND ENGLISH TOKENIZERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsxTvzRAXdRM"
      },
      "source": [
        "# HINDI AND ENGLISH TOKENIZERS  :::::\n",
        "\n",
        "\n",
        "tok_hindi= []\n",
        "tok_english = []\n",
        "\n",
        "co=1\n",
        "\n",
        "def tokenize_hindi(text):\n",
        "\n",
        "  indic_string = text\n",
        "\n",
        "  res=\"\"\n",
        "\n",
        "  for text in indic_tokenize.trivial_tokenize(indic_string): \n",
        "   \n",
        "    #print(text)\n",
        "    text=text.replace('„', r' ')\n",
        "    text=text.replace('“', r' ')\n",
        "    text=text.replace('”', r' ')\n",
        "    text=text.replace('–', r' ')\n",
        "    text=text.replace('—', r' ')\n",
        "    text=text.replace('´', r\" \")\n",
        "    text=text.replace('‘', r\" \")\n",
        "    text=text.replace('’', r\" \")\n",
        "    text=text.replace(\"''\", r' ')\n",
        "    text=text.replace('´´', r' ')\n",
        "    text=text.replace('…', r' ')\n",
        "    text=text.replace('.',r' ')\n",
        "    text=text.replace(',',r' ')\n",
        "    text=text.replace('-',r' ')\n",
        "    text=text.replace('\"',r' ')\n",
        "    text=text.replace('|',r' ')\n",
        "    text=text.replace('।',r' ')\n",
        "    text=text.replace('?',r' ')\n",
        "    text=text.replace('@',r' ')\n",
        "    text=text.replace('_',r' ')\n",
        "    text=text.replace('♫',r' ')  \n",
        "    text=text.replace('♪',r' ')\n",
        "    text=text.replace('!',r' ')\n",
        "    text=text.replace('#',r' ')\n",
        "    text=text.replace('$',r' ')\n",
        "    text=text.replace('%',r' ')\n",
        "    text=text.replace('^',r' ')\n",
        "    text=text.replace('&',r' ')\n",
        "    text=text.replace('*',r' ')\n",
        "    text=text.replace('(',r' ')\n",
        "    text=text.replace(')',r' ')\n",
        "    text=text.replace('/',r' ')\n",
        "    text=text.replace('{',r' ')\n",
        "    text=text.replace('~',r' ')\n",
        "    text=text.replace(':',r' ')\n",
        "    text=text.replace('[',r' ')\n",
        "    text=text.replace(']',r' ')\n",
        "    \n",
        "    res=res+' '+text\n",
        "\n",
        "  reaesc = re.compile(r'[a-zA-Z]')\n",
        "  res = reaesc.sub('',res)\n",
        "  #reaesc = re.compile(r'[#$%^&*()?,/|}-‚.{~:.]।')---this was just making the special characters as a\n",
        "  # separate element in the list and hence does not eliminate it from the list and so removed these\n",
        "  # using the text.replace\n",
        "  #res = reaesc.sub('',res)\n",
        "\n",
        "  res = res.replace(\"''\",r' ')\n",
        "  ### TRIED REMOVING THIS BUT COULD NOT res=res.replace(u'\\u200b',r'')\n",
        "   \n",
        "  return indic_tokenize.trivial_tokenize(res)\n",
        "\n",
        "\n",
        "def tokenize_english(l2):\n",
        "  \n",
        "  l1 = []\n",
        "  reaesc = re.compile(r'♫#?[@_!♫♪#$%^&*(.,)<>?/\\|}{~:]')\n",
        "  text = reaesc.sub('',l2)\n",
        "  text= ''.join(l2)\n",
        "\n",
        "  #print(text)\n",
        "  text=text.replace(\"-\",\" \")\n",
        "  text=text.replace(\"♫\",\" \")\n",
        "  text=text.replace(\"#\",\" \")\n",
        "  text=text.replace(\"?\",\" \")\n",
        "  text=text.replace(\"[\",\" \")\n",
        "  text=text.replace(\"@\",\" \")\n",
        "  text=text.replace(\"_\",\" \")\n",
        "  text=text.replace(\"!\",\" \")\n",
        "  text=text.replace(\"♫\",\" \")\n",
        "  text=text.replace(\"♪\",\" \")\n",
        "  text=text.replace(\"%\",\" \")\n",
        "  text=text.replace(\"$\",\" \")\n",
        "  text=text.replace(\"^\",\" \")\n",
        "  text=text.replace(\"&\",\" \")\n",
        "  text=text.replace(\"*\",\" \")\n",
        "  text=text.replace(\"(\",\" \")\n",
        "  text=text.replace(\".\",\" \")\n",
        "  text=text.replace(\".\",\" \")\n",
        "  text=text.replace(\",\",\" \")\n",
        "  text=text.replace(\")\",\" \")\n",
        "  text=text.replace(\"<\",\" \")\n",
        "  text=text.replace(\">\",\" \")\n",
        "  text=text.replace(\"]\",\" \")\n",
        "  text=text.replace(\":\",\" \")\n",
        "  text=text.replace(\"~\",\" \")\n",
        "  text=text.replace(\"{\",\" \")\n",
        "  text=text.replace(\"}\",\" \")\n",
        "  text=text.replace(\"|\",\" \")\n",
        "  text=text.replace(\"/\",\" \")\n",
        "  text=text.replace(\"?\",\" \")\n",
        "  text=text.replace(\"fuckin'\",\"fucking\")\n",
        "  text=text.replace(\"'ii\",\" will\")\n",
        "  text=text.replace(\"'ll\", r\" will\")\n",
        "  text=text.replace(\"can't\", r\"can not\")\n",
        "  text=text.replace(\"won't\", r\"will not\")\n",
        "  text=text.replace(\"n't\", r\" not\")\n",
        "  text=text.replace(\"'re\", r\" are\")\n",
        "  text=text.replace(\"i'm\", r\"i am\")\n",
        "  text=text.replace(\"'s\" , r\" is\")\n",
        "  text=text.replace(\"'ve\", r\" have\")\n",
        "  text=text.replace(\"'ve\" , r\"have\")\n",
        "  text=text.replace(\"'d\" , r\"would\")\n",
        "  text=text.replace(\"cannot\" , r\"can not\")  \n",
        "  \n",
        "  doc = indic_tokenize.trivial_tokenize(text)\n",
        "  for token in doc:\n",
        "    reg = re.compile(r'[A-Za-z0-9]')\n",
        "    if reg.match(token):\n",
        "      l1.append(token)\n",
        "  \n",
        "  length = len(l1)\n",
        "\n",
        "  list1 = []\n",
        "  for i in range(0,length-1):\n",
        "    if l1[i]=='ai' and l1[i+1]=='not':\n",
        "      l1[i] = \"ain't\"\n",
        "      i=i+1\n",
        "      list1.append(i)\n",
        "  \n",
        "  list2 = []\n",
        "\n",
        "  for i in range(0,len(l1)):\n",
        "     if i not in list1:\n",
        "       list2.append(l1[i])\n",
        "  \n",
        "  return list2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NZANZsW1LSN"
      },
      "source": [
        "PARSING THE CSV INTO HINDI AND ENGLISH LISTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4_iKGC7TADw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb098f54-ff1a-426f-e129-e208e67bf54c"
      },
      "source": [
        "#!cut -d, -f1 --complement /content/train.csv > /content/new_train.csv \n",
        "#statements = open('/content/new_train.csv').read().strip().split('\\n')\n",
        "\n",
        "\n",
        "hindi_list= []\n",
        "english_list = []\n",
        "\n",
        "\n",
        "with open('train.csv', 'r') as file:\n",
        "  reader = csv.reader(file)\n",
        "  c=0\n",
        "  res=0\n",
        "  k=0\n",
        "  for row in reader:\n",
        "    m = row[1]\n",
        "    hindi_list.append(m)\n",
        "    if \"8\" in m:\n",
        "      print(m)\n",
        "      res=res+1\n",
        "    if \"9\" in m:\n",
        "      q=q+1\n",
        "    c=c+1\n",
        "    english_list.append(row[2])      \n",
        "\n",
        "  print(res)\n",
        "  print(q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28 साल पुराना है.\n",
            "\"17 जनवरी, 1989,\n",
            "100 करोड़ बच्चों के लिए हमें 10 करोड़ मांझी चाहियें -- इस धरती पर उससे कहीं ज्यादा हैं -- एक करोड़ सोल्स, 18 हज़ार करोड़ डॉलर और दस साल.\n",
            "हम बस के साथ महानगरों 8 खबर कनेक्शन खो दिया है।\n",
            "उह उह, 20 THOUSA-- 60 - 60,850.\n",
            "आप जुड़वां टर्बो वी 8 के बारे में बात कर रहे हैं, 560 टट्टू, बेटा बाहर थूकना.\n",
            "जलवायु परिवर्तन, 2008 का महान वित्तीय संकट दो सरल उदाहरण हैं।\n",
            "2008 में, एक गूगलर, सुंदर पिचई, एक उद्देश्य पर ले गया अगली पीढ़ी के ग्राहक मंच का निर्माण वेब अनुप्रयोगों के भविष्य के लिए - सर्वश्रेष्ठ ब्राउज़र\n",
            "अब तुम हमें हमारी 80 देना भव्य पीठ और उसे अपने साथ ले जाओ.\n",
            "971 01: 47: 13,583\n",
            "जेजी: या यहाँ एक वीडियो है \"क्राश कोर्स\"के हिस्से के रूप में: प्रथम विश्व युद्ध के बारेमे (वीडियो) तत्काल कारण ज़ाहिर था ऑस्ट्रियन आर्कड्यूक फ्रांज फर्डिनेंड केसाराजेवो में हत्या, 28जून1914पर,गाव्रिलो प्रिंसिप नाम का एक बोस्नियाई सर्ब राष्ट्रवादी द्वारा\n",
            "औरत [टीवी पर]: रदरफोर्ड उम्रकैद की सजा काट रहा था... ...एक 1968 शूटिंग के लिए.\n",
            "एक के आगे 18 जीरो.\n",
            "हमें पता चला कि कटु बर्ताव लोगों का हौसला और भी कम कर देता है: 66 प्रतिशत लोगों ने काम में मेहनत करना कम कर दिया, 80 प्रतिशत लोग सोच में पड़ जाते कि इन चीज़ों का कारण क्या है, और १२ प्रतिशत लोगों ने नौकरी छोड़ दी।\n",
            "और इसे हाल में 2008 के अंत में ही देखा गया है. इसी क्षेत्र में दोबारा इस स्थान पर आधे भूभाग में अंधेरा है क्योंकि दक्षिणी गोलार्ध में अगस्त के आगमन के साथ ही अब शीतकाल शुरू हो गया है.\n",
            "लेकिनवहएक अवरोधन और एक 38 यार्ड वापस मिल गया है\n",
            "हमें इतिहास को बताने के नए तरीकों की ज़रूरत है, और इस साल, उदाहरण के लिए, हमने एक नई ऑनलाइन प्रोजेक्ट शुरू किया जिसे 1 9 68Digital.com कहा जाता है, और यह एक ऑनलाइन डाक्यूमेंट्री श्रृंखला है जो आपको वर्ष 1 9 68 का एक इंप्रेशन देता है जो विश्व सामाजिक परिवर्तन द्वारा चिह्नित एक वर्ष है, इसने कई मायनों में, दुनिया को बनाया जैसा हम इसे अब जानते हैं।\n",
            "$ 248.\n",
            "हाँ, यह है 78 रोमली एवन्यू।\n",
            "किसी ने मुझे बता सकते महत्व तारीख की 21 जुलाई 1861?\n",
            "ये किसी चुनाव के आंकड़े नहीं हैं, ये उन व्यक्तियों की संख्या है जो हर राज्य में मोटापे का शिकार हैं क्रमशः 1985, 1986, 1987 के -- ये CDC की वेबसाइट से लिए गए हैं -- फिर 1988 से लेकर 1991 तक -- हमारे सामने नई श्रेणी आ गई है -- फिर 1992 से 1996 क्रमशः 1997 से लेकर 2001 तक -- स्थिति गंभीर हो रही है. इस सबके जिम्मेदार हम ही हैं. अब हम क्या कर सकते हैं? हमने यह पाया है कि एशियन आहार को प्रोत्साहन देकर हम हृदयरोग और कैंसर में कमी ला सकते हैं.\n",
            "वी 8.\n",
            "फ़ॉन्ट रंग = \"# 808080 \" ड्यूक:\n",
            "1989 में दीवार गिरने के बाद, इस इमारत से हमने पूर्वी से पश्चिम बर्लिन काे जोड़ा पॉट्सडमर प्लैट्ज में.\n",
            "प्रकृति के विषयों के मेरे निरंतर अन्वेषण में सब कुछ एक दूसरे से सम्बंदिथ होने का उदाहरण देता है मैंने 2008 में तूफानों का पीछा करना शुरू किया था मेरी बेटी के कहने पर, \"माँ, तुम्हे यह करना चाहिए ।\"\n",
            "और ग्राफ यह करता है, जिसका महत्वपूर्ण तर्क , समय के साथ यह असाधारण वृद्धि है, यही कारण है कि 20 वीं शताब्दी को लोकतंत्र की जीत शताब्दी कहा जाता है, और क्यों, 1 9 8 9 में फ्रांसिस फुकुआमा ने कहा, कुछ का मानना ​​है कि हम इतिहास के अंत पर पहुंचे हैं, कि एक साथ रहने के प्रश्न का उत्तर दे दिया गया है, और वह उत्तर उदार लोकतंत्र है।\n",
            "और 1.8 अरब युवा लोग इंतजार कर रहे हैं ।\n",
            "ग्रीनलैंड रहता है। 28 साल पहले, ध्रुवीय बर्फ़ की चोटी इस तरह थी उत्तर ध्रुवीय बर्फ़ की चोटी- विषुव में गर्मियों के अंत में इस तरह दिखती थी।\n",
            "पिछले साल नवंबर में 2012, 85 विभिन्न देशों से 33,000 से अधिक धावकों पिछले साल नवंबर में 2012, 85 विभिन्न देशों से 33,000 से अधिक धावकों ने हिस्सा लिया, लेकिन इस बार, वे चुनौती दे रहे थे, बहुत ही तूफानी और बरसात के मौसम को।\n",
            "18 के माध्यम से 10 का उपयोग.\n",
            "यदि आप अपने दोस्त को फिर से देखना चाहता हूँ, तुम मुझे मेरे 80 हज़ार मिलता है.\n",
            "रॉबलिंग को आत्मविश्वास था कि उनकी बनावट काम करेगी, परन्तु 1869 में स्थल का सर्वेक्षण करते हुए, एक आती हुई नाव ने बंदरगाह पर उनका पैर कुचल दिया।\n",
            "मुख्य विज्ञान सलाहकार के रूप में 100 दिनों के बाद, नेमेर ने 2018 के लिए अपने कार्यालय की योजनाओं को रेखांकित करते हुए एक पत्र जारी किया, जिसमें वैज्ञानिक अखंडता नीतियों और दिशानिर्देशों को विकसित करना शामिल है, यह सुनिश्चित करने के लिए दिशा-निर्देशों की सिफारिश की गई है कि सरकारी वैज्ञानिक अपने शोध के बारे में खुलकर बात कर सकते हैं और खुली जनता की अनुमति देने के लिए एक रूपरेखा तैयार कर सकते हैं संघीय सरकार विज्ञान तक पहुंच।\n",
            "पर 2008 के उस चुनाव के बाद फिर कुछ और हुआ।\n",
            "वैश्विक पर्यावरण संस्थान, एक चीनी नागरिक समाज समूह, ने पाया कि पिछले 15 वर्षों में, चीन ने और अधिक निवेश किया है 240 कोयले के बिजली उत्पादन संयंत्रों से 68 से अधिक देशों में वन बेल्ट एक सड़क इनिशिएटिव से संबद्ध।\n",
            "इन युवाओं की उम्र 18 से 28 के बीच है |\n",
            "पल्स 78 पर स्थिर.\n",
            "पौधो की विकृति विज्ञान के जनक अन्तोन डी बेरी ने 1884 मे फळे व्यक्ती थे जीन्होने माना फफूंदी उन संकेतो द्वारा मार्गदर्शन पाते है . जो मेजबान पौधे द्वारा भेजे जाते है अर्थात उस पौधे से जिस पर फफूंदी रह सके और उसे खा सके यानी संकेत एक प्रकाश स्तंभ का काम करते है ताकि फफूंदी उन्हे ढूँढ सके, और उसकी तरफ बढ़ सके, पहुँच सके और आख़िरकार उस पर हमला कर उसे अपने उपनिवेश बना सके\n",
            "पहली बार मैं हैती अगस्त,2008 में गया, एक तरह से झक मारने, व मैंने देश के ग्रामीण दक्षिण में ऊर्जा गरीबी आंकलन सर्वे किया। ऊर्जा गरीबी का आंकलन करने के लिए।\n",
            "एप्पल ने 1987 में हाइपरकार्ड का आविष्कार किया।\n",
            "कंट्रोल 486।\n",
            "1989 में क्रिसमस की शाम को, लाइबेरिया में गृह युद्ध छिड़ गया।\n",
            "उन्होंने तब उन सभी 80 बच्चों को दिया। एक मानकीकृत गणित परीक्षण\n",
            "दुनिया के 80 प्रतिशत लोगों के लिए भुखमरी से बचानेवाला कोइ सुरक्षा जाल नहीं है.\n",
            "83 से 95 तक :दो प्रतिशत हर साल\n",
            "(हँसी) अब, 1986.\n",
            "फिर, 2011 में, मैंने कोर्ट स्क्वायर पार्क में - छह प्रवेश उद्यान लगाये, 80 किस्मों के सुगंधित फ्लोरिबुंडा को और संकर चाय गुलाब।\n",
            "38 पर 5 लो.\n",
            "बहुत सारे। यही कोई 80 के करीब।\n",
            "तो 2008 की उस रात से बहुत कुछ बदल चुका है जब मैं दो टुकड़ों में चिर गई थी।\n",
            "और इस तरह के आंदोलन अविश्वसनीय रूप से प्रभावी और लोकप्रिय हो गए है, तो 1985 में उनके विरोधियों ने हमारा नज़रिया बदलने के लिए एक नये शब्द का ईजाद किया -\n",
            "3 सितंबर 1989\"\n",
            "अब ,उस बेन्च पर 82 वर्षीय महिला बैठी है, जो समाज में ग्रेंडमदर जैक के नाम से जानी जातीं हैं ।\n",
            "जब मैं 18 का हुआ, तब मैंने अपने सबसे अच्छे मित्र को सड़क दुर्घटना में खो दिया|\n",
            "टोंगा गर्त (21,582 फुट) अमेरिका UUV अल्फा 1 की संपत्ति\n",
            "हंसी ! उन्होने 1862 दिसंबर में यह कहा कांग्रेस की दूसरी वार्षिक बैठक को\n",
            "1877 तक, निर्माण कार्य आय-व्ययक से ऊपर और निर्धारित समय से पीछे था।\n",
            "हैती में 2008 के तूफानी मौसम की वजह से लगभग एक अरब डॉलर की क्षति हुई।\n",
            "आपमध्य70 में धूप highs से उम्मीद कर सकते हैं और यह 78 के रूप में भी रूप में उच्च तक पहुँच सकता है\n",
            "हम 18 थे.\n",
            "4O कमांडो गेट उल्लंघन किया और 28 रह गए हैं.\n",
            "सन 2011 में, एक सुनामी ने उस बाढ़ दीवार को लांघ दिया था जो जापान के फुकुशिमा परमाणु ऊर्जा संयंत्र की रक्षा के लिए बनायी गयी थी, जिससे परमाणु आपदा पैदा हो गयी थी और 18,000 से ज़्यादा लोगों की मौत हो गयी थी।\n",
            "[ध्यान रखें, इस व्याख्यान में परिपक्व भाषा का उपयोग किया गया है।] अगर हम सन 800 बी सी के ग्रीस में वापस जाएँ, तो हमें पता लगेगा कि उस समय में जब व्यापारियों का कारोबार असफल होता था, उन्हें बाज़ार में अपने सर पर एक टोकरी लेकर बैठने पर मजबूर किया जाता था।\n",
            "और उसके बाद इस साल (2008) के अंत -- ये शानदार स्टेल्थ वर्ज़न है, जिसमें बहुत अच्छी डिज़ाईन क्षमताएँ हैं, अब देखिए -- प्लैंक उपग्रह को प्रक्षेपित किया जाएगा, जो बहुत अधिक रिज़ोल्यूशन के मानचित्र उपलब्ध कराएगा.\n",
            "आ ब: 3846264338327950 28841 ...\n",
            "28 देशों में मार्शल कानून घोषित किया जा चुका है... अमेरिका और कनाडा में भी।\n",
            "और फिर , कहता है , 1986, बासित और अमजद.\n",
            "हाना रीट्श युद्ध में बच गयी और बनाती रही कई विश्व उड़ान रिकॉर्ड. उसका 28 अगस्त, 1979 को निधन हो गया.\n",
            "नागरिक सुबह 9 बजे से रात के 8 बजे तक कभी भी मशीन से जाकर जितना पानी चाहो ले सकते हो, ताकि उन्हें रोज़ बोतल से पानी लेने के लिए लम्बी लाइन में खड़ा न होना पड़े।\n",
            "तो यह बहुत स्पष्ट है कि अगर आप इन नंबरों को देखो या अन्य सभी संख्या है कि मैं अपनी किताब में के बारे में बात करते हैं, \"विश्व 3.0,\" कि हम बहुत, बहुत से दूर रहे हैं कोई सीमा प्रभाव बेंचमार्क, जो संकेत होगा 85 के आदेश के अंतरराष्ट्रीयकरण का स्तर, 90, 95 प्रतिशत.\n",
            "हमने देखा है 2008 के दंगों को जो खाने के अभाव से हुए, जब मेरे शब्दों में, भूख के एक खामोश बाढ़ ने दुनिया को लपेट लिया था, जब रातों रात खाद्य-पदार्थों के दाम दुगुने हो गए थे.\n",
            "(ठहाके) दिल्ली से लगभग 8000 किलोमीटर दूर गेट्सहैड नामक एक छोटा शहर है.\n",
            "बेशक हमनें 2008 में यही देखा.\n",
            "हम अपने लड़कों के 8 लोगों की है\n",
            "- क्या तुम 18 साल के ऊपर हो?\n",
            "हमने 1982 से लेकर हर साल एक से ज़्यादा नई तरह के वायु-यान बनाए हैं.\n",
            "और यह 1980 तक इसी रूप मेँ रहा, जब ये अवधारणा को चिकित्सा समुदाय और सार्वजनिक स्वास्थ्य नीति निर्माताओं द्वारा चुनौती दी गयी तो उनको अहसास हुआ कि सब चिकित्सा अनुसंधान अध्ययन से महिलाओँ को बाहर करने से हम वास्तव मेँ उनको एक नुकसान कर रहे हैँ, उस मेँ प्रजनन मुद्दों को छोडकर, महिला रोगी की अद्वितीय जरूरत के बारे मेँ लगभग कुछ भी नही जाना जाता था|\n",
            "अब ६५० मिलियन हैं डिजिटल समाचार उपभोक्ताओं, ५८० मिलियन डिजिटल वीडियो उपभोक्ता, और देश का सबसे बड़ा ई-कॉमर्स मंच ताओबाओ अब 580 मिलियन मासिक सक्रिय उपयोगकर्ता दावा करते है\n",
            "हमने इसके बजाय केंद्रों की स्थापना की है जिनकी कार्य अवधि समय स्कूलों की तुलना में अधिक है, इनमें आबादी के सभी लोग शामिल हैं - हमारा सबसे छोटा उपयोगकर्ता 3 साल का है, सब से वृद्ध 86 साल का है - और इससे हम दो वर्षों से भी कम समय में, 140,000 उपयोगकर्ताओं तक पहुँच पाए थे, जिसमें से -- (तालियाँ) आपका धन्यवाद।\n",
            "आप प्रति माह 800 रखना.\n",
            "2008 की चुनावों की रात एक ऐसी रात थी जिसने मुझे दो हिस्सों में चीर दिया।\n",
            "04:08\n",
            "मैंने क़रीब दो सौ ऐसी पेंटिंग बनाईं, कुछ तो 6 से 8 फ़ीट लम्बी, जैसे की ये।\n",
            "अरे नहीं. शानदार 80.\n",
            "चप्पू 88 तक बिक गया.\n",
            "निकटतम मैं व्हीलिंग है प्राप्त कर सकते हैं, और है कि उड़ान 08: 00 तक नहीं छोड़ता.\n",
            "उस किताब से दूसरी किताब बनी, \"ब्लेक फोटोग्राफर्स, 1940-1988,\" और उस किताब से दूसरी किताब, और फिर एक और किताब, फिर एक और किताब,फिर एक और किताब, फिर एक और किताब,फिर एक और किताब, फिर एक और किताब,फिर एक और किताब, एक और किताब,फिर एक और किताब, फिर एक और किताब,फिर एक और किताब, एक और किताब, फिर एक और किताब, फिर एक और किताब, फिर एक और किताब,एक और\n",
            "क्योंकि यह सन 1983 है, हम इतने सामजिक अछूत नहीं जिन्हें... आप शायद जानते और नफ़रत करते हैं, पर हमें समय दीजिए।\n",
            "48 चैनल, मैग मौंटेड डीवी27 एंटीना के साथ पूरा।\n",
            "80 के दशक मे वे बिल्कुल भयानक थे मैं वध संयंत्रों के लिए एक बहुत सरल स्कोरिंग प्रणाली विकसित की\n",
            "1 858 से एक है?\n",
            "और दोनों का मेल 80 प्रतिशत बच्चों को 3 महीनों में ठीक कर देता है |\n",
            "अंग्रेज़ों द्वारा 1814 में व्हाइट हाउस को जलाने की\n",
            "Rects1-8-0का समन्वय कर रहे हैं में, .\n",
            "8 बजे मुझे उठाओ.\n",
            "इसलिए, स्कॉटलैंड ने, 2018 में, एक नए नेटवर्क, वेलबींग इकॉनमी गवर्नमेन्ट्स ग्रुप बनाने का नेतृत्व लिया, और संस्थापक सदस्य देश स्कॉटलैंड, आइसलैंड, और न्यू ज़ीलैंड को साथ लाया, ज़ाहिर सी वजहों के लिए\n",
            "जनगणना ब्यूरो का कहना है कि 2050 तक, इस देश में 84 मिलियन वरिष्ठ नागरिक होंगे।\n",
            "इसमें 14 वर्ष, आज के 40 करोड़ डॉलर, और तीन अलग अलग रॉबलिंग के जीवन भर का कार्य लगा परन्तु जब 24 मई 1883 को ब्रुकलिन पुल आखिरकार खुला तो उसका वैभव निर्विवाद था।\n",
            "फ़ॉन्ट रंग = \"# 808080\" वार्डन:\n",
            "22 मार्च को, भारत सरकार ने देश के 22 राज्यों और केंद्र शासित प्रदेशों के 82 जिलों को पूरी तरह से बंद करने का निर्णय लिया, जहां 31 मार्च तक पुष्टि होने की खबर है। 23 मार्च को दिल्ली में कम से कम 31 मार्च तक लॉकडाउन करने का निर्णय लिया।\n",
            "शायद 8, या 9, या 10।\n",
            "मुझे ये सुनकर तक़लीफ होती थी, पर तब मैंने सोचा, एक मिनट. मैंने अपना पहला एपल कंप्युटर 1978 में ख़रीदा था और इसलिए ख़रीदा था ताकि मैं कह सकूँ कि, \"मेरे घर में कंप्युटर है और तुम्हारे घर नहीं.\n",
            "आपको एक और उदाहरण देती हूं: 2008 में, यूसीएलए में एक पैटर्न देख रहे थे शहर में जहां चोरी ज्यादा हो रही थी.\n",
            "(हँसी) आठ दिनों के भीतर, मुझे अपग्रेड करना पडा होस्टिंग पैकेज आठ बार, सबसे नीचे के पैकेज से सबसे महंगे पैकेज के लिए, और अब, लगभग चार साल बाद, मेरे पास सफेद सूची पर 8,000 नाम हैं दुनिया भर से।\n",
            "दुनिया भर में अस्सी लाख अनुमानित बच्चे अनाथालय जैसी संस्थाओं में रह रहे हैं, इस तथ्य के बावजूद कि उनमें से लगभग 80 प्रतिशत अनाथ नहीं हैं।\n",
            "अच्छे से जुड़ा हुआ, 800 साल पहले, यूरोप से।\n",
            "और मेरे लिए अचरज की बात है यह सोचना कि 1840 में,अफरिकी अमेरिकी फोटो खिंचते थे।\n",
            "और आप वास्तव में देख सकते हैं कि समारा के लाइसेंस प्लेट 78RUS में समाप्त होता है.\n",
            "यह मेरा सबसे पसंदीदा दर्शन है, यह फ्लाईओवर जहाँ आप 8 फीट द हाई लाइन की तह से ऊपर हैं, वृक्षों के एक मंडप से गुज़रते हुए.\n",
            "क्या तुम पर देख रहे हैं 18 FMJ स्पिट्जर गोली से एक 5.45 है.\n",
            "एक ही पल में 178 यात्रियों kaput\n",
            "तो हर उस चित्र के लिये जो आप देख रहे हैं, और ऐसे लगभग 1800 चित्र हैं इस फोटो मे जब भी मैं फोटो खींचता हमे अपने पैर एक स्थान पे चिपकाने पड़ते.\n",
            "वे निम्न-आय वाले देशों में भोजन का 60 से 80 प्रतिशत उत्पादन करती हैं, अक्सर पांच एकड़ से कम पर काम कर रही है।\n",
            "उसका आकिन में 21 दिसंबर, 1998 को निधन हो गया.\n",
            "मैं 28 हूँ.\n",
            "तो, निर्माण में भारी चीज़ें, रियल एस्टेट, बंधक बाज़ारों। वित्तीय सेवाओं: हमारे पास 89 बैंक थे। बहुत से अपने वास्तविक व्यापार भी नहीं कर रहे थे।\n",
            "आप 180 डिग्री जागरूकता रखने?\n",
            "धर्म के युवा अनुचरों को तीन या चार वर्ष ही आयु में ही उनके परिवार से अलग कर उन्हें 18 वर्षों तक बर्फीली चट्टानों में बने पत्थर के झोपड़ों में अंधकार में रखा जाता है । नौ महीनों की दो अवधियां जान-बूझकर चुनी जाती हैं क्योंकि ये गर्भधारण के नौ महीनों के समान दर्शायी जाती हैं । जिस दौरान वे नौ महीनों तक प्राकृतिक तौर पर\n",
            "1860 में सिर्फ़ कपास की बिक्री लगभग 200 मिलियन डॉलर के बराबर थी।\n",
            "38 वीं मंजिल!\n",
            "-8\n",
            "देखो, तुम्हारे दिमाग में इस समय 80 अरब न्यूरॉन मौजूद हैं।\n",
            "जब हम शरणार्थियों के मुद्दे पर बात करते हैं, हम अक्सर सरकारी आँकड़ों पर ध्यान केंद्रित करते हैं की 65.8 मिलियन जबरन दुनिया भर में विस्थापित।\n",
            "\"सन 1978 में, हेल्थ डिपार्टमेंट द्वारा प्रकाशित एक पुस्तिका \"\"द हेल्थ हैज़ार्ड ऐट वाइटेनूम (The Health Hazard at Wittenoom)\"\", जिसमें हवा के नमूनों के परिणाम और विश्वव्यापी चिकित्सा जानकारी की एक समीक्षा थी, के प्रकाशन के बाद वेस्टर्न ऑस्ट्रेलिया की सरकार ने विटेनूम नगर को हटाने का निश्चय किया।\"\n",
            "48 घंटे की एक अंतराल .. at।\n",
            "उस मौसम की अंत् मेँ वे एक बीज उत्पादन करते हैँ, जो सूखा हो, 8 से10 प्रतिशत पानी हो, लेकिन एकदम जिंदा हो|\n",
            "(तालियाँ) लेकिन जब मैं 15 साल का हुआ तो वीड्स बेचने लगा, मैंने हाई स्कूल पूरा नहीं किया, न्यूयॉर्क एजूकेशन बोर्ड ने मुझे सभी स्कूलों में बैन कर दिया, लेकिन 18 साल का होने पर मैने कोकीन बेचने के लिए ग्रेजूएशन की।\n",
            "पता लगा कि मेरे अंदाज़े से कहीं अधिक इसकी आवश्यकता थी , क्योंकि 48 घंटे के भीतर, मुझे 750 ईमेल मिले\n",
            ".. 9, 8, 7, 6 .. कृपया, आप के बाद.\n",
            "तीन-सितारा, 48 चैनल।\n",
            "आप जानते हैं, 98, 99 प्रतिशत लोग इससे जीवित बाहर निकलने जा रहे हैं।\n",
            "फ़ॉन्ट रंग = \" # 808080 \" अंधी गली:\n",
            "यह सहमति हुई कि सभी शत्रुता खत्म होती है 8 मई की आधी रात को.\n",
            "और फिर, 1980 के दशक के आसपास, यह अपनाया गया हिप-हॉप, बी-लड़कों, स्केटबोर्डर्स द्वारा और इस तरह से यह युवा सड़क संस्कृति में शामिल हुआ।\n",
            "सन 1987 में मैं पहली बार मां बनी. मेरा पहला बच्चा हुआ, और मैं उस को दूध पिला रही थी, जब इससे बहुत मिलता-झुलता एक चित्र दूरदर्शन पर प्रसारित हुआ.\n",
            "हाँ, लेकिन आप देखते हैं, आप उह देख ... 9-2-5 करने कूदना होगा है कि 8-5-6,.\n",
            "1989 के अप्रैल में इस वायरस की खोज के बाद इसका नया नाम हेपेटाइटिस सी वायरस, एचसीवी (HCV)साइंस के जर्नल में दो लेखों में प्रकाशित हुआ था.\n",
            "जब यह चित्र 1988 में लिया गया था, कोई कभी भविष्यवाणी नहीं कर सकता था आज हम जहाँ होंगे।\n",
            "318?\n",
            "फ़ॉन्टरंग=\" # 808080\"ड्यूक:\n",
            "3.14159265358979 - रेजी वाट्स: यदि हमे कुछ करना है, तो हमें निर्णय लेने की ज़रूरत है|\n",
            "मुख्य भूकंप के सात घंटों के भीतर 30 से अधिक झटकें दर्ज किए गए, वैसे तो भूकंप के साथ जुड़े झटकों की संख्या 100 से अधिक थी पर अधिकांश 2.0 परिमाण से कमजोर थे। 26 मार्च के अंत तक, भूकंप के बाद 1.3 मेगावाट से अधिक या बराबर के 106 झटकें पाए गए। जिनमें से 8 की तीव्रता 3.0 या उससे अधिक थी। क्रोएशिया की भूकंपीय सेवा के अनुसार सबसे बड़ा 6:01 UTC मिनट में 4.8 मेगावाट की घटना थी।\n",
            "पहला जुलूस 1979 में निकला और दूसरा 1987 में।\n",
            "विश्व स्वास्थ्य संगठन (WHO) विश्व के देशों के स्वास्थ्य संबंधी समस्याओं पर आपसी सहयोग एवं मानक विकसित करने की संस्था है। विश्व स्वास्थ्य संगठन के 194 सदस्य देश तथा दो संबद्ध सदस्य हैं। यह संयुक्त राष्ट्र संघ की एक अनुषांगिक इकाई है। इस संस्था की स्थापना 7 अप्रैल 1948 को की गयी थी। इसका उद्देश्य संसार के लोगो के स्वास्थ्य का स्तर ऊँचा करना है। डब्‍ल्‍यूएचओ का मुख्यालय स्विटजरलैंड के जेनेवा शहर में स्थित है। इथियोपिया के डॉक्टर टैड्रोस ऐडरेनॉम ग़ैबरेयेसस विश्व स्वास्थ्य संगठन के नए महानिदेशक निर्वाचित हुए हैं।\n",
            "कम से कम 18 लोग ड्यूसबर्ग की लव परेड में मची भगदड़ में पैरों की ठोकर से या कुचलकर मर गए हैं.\n",
            "आप को समाप्त जीत बस के नीचे शानदार 80.\n",
            "आप मुझे एक बहुत बियर के बिना 800 मील की सवारी करने की उम्मीद है?\n",
            "यदि तपेदिक फिर से होता है तो, उपचार का निर्धारण करने के पहले इस बात का परीक्षण करके निर्धारण कर लेना चाहिये कि यह किस एंटीबायोटिक के प्रति संवेदनशील है। यदि एक से अधिक दवा प्रतिरोधी टीबी (एमडीआर - टीबी) का पता चला है तो 18 से 24 महीनों के लिए कम से कम चार प्रभावी एंटीबायोटिक दवाओं के साथ उपचार की सिफारिश की जाती है।\n",
            "इसका व्यास 8 मीटर का है, तो आप सोच सकते हैं कि विशालकाय दूरबीनें आप इसमें ले जा सकते हैं, जिनसे कई अभूतपूर्व चीज़ों को देख पाएंगे, और रोचक खोजें संभव होंगी.\n",
            "मैं 1983 के बाद से एक महिला को नहीं मारा.\n",
            "8 साल बाद, इंसाइट को बंद करने की लड़ाई कनाडा के सुप्रीम कोर्ट तक पहुंची.\n",
            "(खिलखिलाहट) मई 1968 के कुछ साल बाद की बात है\n",
            "कोई फायदा नहीं, उसका काम 8 बजे समाप्त होता है.\n",
            "1980 के दशकों में खींच ले जाऊँगा।\n",
            "यह सामने आया कि फिट्ज़पैट्रिक का घरेलू जीवन एक जीवंत नर्क था। उसने बचपन की दोस्त मौरीन से शादी की... जब वह 18 का था पर समय बीतने पर वह काफी मोटी हो गई...\n",
            "और उड़ान 8 पर छोड़ देता है।\n",
            "शायद वे मुझसे आप के बारे में पूछेंगे, वे लोग जो 2018 में जीवित थे ।\n",
            "डेरिअस ने पेल्टिअर टी शर्ट के बारे में पूछा जिसे मैं 80 के दशक में बहुत पहनता था, जो कि, अफ़सोस है, आज भी प्रासंगिक है।\n",
            "Dink, ग्रह जी -8 के पुल के िलए क्षतिपूर्ति और पूर्ण गला घोंटना जाओ!\n",
            "1868 में पहला ग्रैंड विज़ार्ड, नाथन बेडफोर्ड फोरेस्ट, एक संघीय सैनिक और एक करोड़पति दास व्यापारी था।\n",
            "लखपतियों का एक सर्वे किया गया उनके जीवन के सबसे महत्वपूर्ण लक्ष्य क्या थे, उनमे से 80% ने कहा था कि जीवन का सबसे बड़ा लक्ष्य था अमीर बनाना\n",
            "जेम्स ने सैन क्वांटिन राज्य जेल में 12 साल की सज़ा काटी है और 18 महीनों में रिहा होने वाला है।\n",
            "यह एक 1948 रोल्स रॉयस सिल्वर रेथ है।\n",
            "(हँसते हुए) और 80 के दशक में, वहाँ बंगलादेश अफ्रीकी देशों के बीच में था।\n",
            "रबड़ को जोड़ने का काम 1858 में हुआ, हाईमन लिपमैन ने पहली पेंसिल पेटेंट करवायी जिसमें रबड़ लगा था, जिसने पेंसिल के मायने ही बदल दिये।\n",
            "ये महिलाएं -- भगवान भला करे इनका -- अपना हर दिन हज़ारों बच्चों के लिए खाना बना कर निकलती हैं, नाश्ता और दोपहर का खाना, सिर्फ $२.६८ (180₹) प्रति भोजन, जिसमे से खाने के सामान की तरफ सिर्फ एक $ (70₹) जाता है |\n",
            "यह हमारा पहला प्रयोग था -- आपके दाहिने ओर एक 8 साल का लड़का एक 6 साल की लड़की को browse (ब्राउस) करना सिखा रहा था.\n",
            "संरचनात्मक क्षति, 80%.\n",
            "प्रत्येक का वजन 8 किलोग्राम है।\n",
            "08:00\n",
            "और अगर वे अपनी माल आपूर्ति, समावेशन और जिम्मेदारी के साथ करती हैं - तो उनके मुनाफे में 4.8% बढ़ोतरी देखी गयी.\n",
            "यहाँ आप कोबे (COBE) उपग्रह को देख सकते हैं, जिसे 1989 में छोड़ा गया था, और जिससे इन अनियमितताओं की खोज हुई.\n",
            "वे एक दूसरे के साथ संवाद करते हैं दो सिग्नलिंग अणुओं के माध्यम से जिन्हे इंटरलेकिन -6 और इंटरलेकिन -8 कहा जाता है।\n",
            "इसलिए पिछले वर्ष 26 अक्टूबर को, जेल में 18 वर्ष काटने के बाद, मैं राष्ट्रपति के क्षमादान पर जेल से बाहर निकला।\n",
            "इस सब के साथ, कि 82,400 डॉलर है.\n",
            "ये हमारा सबसे बड़ा हवाई जहाज है, एयरबस A380, और यह काफी बड़ा है, इसलिए इसमें बहुत से लोग समा जाते हैं और यह तकनीकी रूप से उस कल्पना से पूरी तरह से अलग है जो मैंने आपको दिखाई थी ।\n",
            "उसने 1.8 करोड़ यूरो का काला संग्रह खरीदा है जो फ़ैशन डिज़ाइनर वायेस सैंट लौरेंट का होता था, बहुत सारी शानदार गाडियाँ, जिनमे से कुछ एक की कीमत 10 लाख डॉलर्ज़ है -- ओह, और एक निजी हवाई जहाज़ भी.\n",
            "यह अपने शरीर को 180 डिग्री से पलटता है | यह गिरने से बचने के लिए दो पैरों के बीच से पैर को घुमाता है |\n",
            "कोरिया में इसे 18 दिन लगे।\n",
            "यहाँ तापमान 210 डिग्री से अधिक कृत्रिम रूप से एक कुरकुरा 68 ठंडा करने के लिए नहीं है.\n",
            "जिप्सी खतरा, खाड़ी 08, स्तर A42 के लिए रिपोर्ट.\n",
            "ठीक है, तुम 8: 00 पर में जाओ, बस के रूप में वह खुल जाता है.\n",
            "क्या आप 8, 10, 14 वर्ष की उम्र में भी उनके लिए सब कुछ ठीक करते हैं?\n",
            "हमारे वर्तमान गिनती खड़ा 7811 में, Stannis Baratheon के बेड़े को जलाने के लिए पर्याप्त\n",
            "ईगल 18 एकड़ पर रहेंगे।\n",
            "पर जब मैं उठा छत, मुझे पता हुआ ... Bellagio की 80,000 डॉलर मूल्य मेरी जेब में चिप्स.\n",
            "और मैंने जाना कि कुछ साल बाद, 1998 में, एक संपूर्ण सूर्य ग्रहण कैरिबियन से गुज़रने वाला था।\n",
            "एक वर्ष, 18 महीने, क्या यह संभव है?\n",
            "हम मुद्रास्फीति की दर को 28 प्रतिशत से लगभग 11 प्रतिशत नीचे लाये।\n",
            "800 डॉलर 11: 05.\n",
            "मैं बुधवार 8 बजे समाप्त पाली, ठीक है?\n",
            "विकासशील देश अब हमारा अनुसरण कर रहे हैं और अपनी गति बढा़ रहे हैं। और वास्तव में, उनके संचयी उत्सर्जन इस वर्ष उतने ही हैं जितने हमारे 1965 में थे। वो नाटकीय रुप से आगे बढ़ रहे हैं सन्पूर्ण सान्द्रता 2025 तक वो वहाँ होंगे जहाँ हम 1985 में थे\n",
            "क्या आप कल्पना कर सकते हैं कि कितने पहली बार उद्यमी होंगे 84 मिलियन लोगों के बीच?\n",
            "यह वही है जो मार्च 1989 में घटित हुआ था कनाडा के क्यूबेक प्रांत में, जब बिजली संचरण ने शक्ति खो दी।\n",
            "हालांकि, दोपहर के बाद जब तापमान कम होना शुरु हुआ, पुलिस ने अपनी नई शक्तियों का प्रयोग करते हुए 968 लोगों को पकड़ लिया.\n",
            "यह एक गलत मत-संख्या की वजह से हुआ था जिसके अनुसार अफ़्रीकी-अमेरिकियों ने प्रस्ताव 8 के लिए 70 प्रतिशत मत डाले थे।\n",
            "हम उद्यमियों के स्वर्णिम राज्य योद्धा की तरह हैं - (हँसी) (तालियां) और यह संख्या युवा उद्यमियों के लिए 28 प्रतिशत है।\n",
            "उसका 1981 में लंदन में निधन हो गया.\n",
            "मैं गारंटी, के 8 से 10 प्रतिशत अपने निवेश करना होगा\n",
            "[[File:TB incidence.png_thumb_left_नये दर्ज किये गये टीबी के मामलों की वार्षिक संख्या डब्ल्यूएचओ के आंकड़े विकसित देशों में, तपेदिक कम आम है और मुख्य रूप से शहरी क्षेत्रों में पाया जाता है। 201 में दुनिया के विभिन्न क्षेत्रों में प्रति 100,000 लोगों की दरें : विश्व स्तर पर 178, अफ्रीका 332, अमेरिका 36, पूर्वी भूमध्य 173, यूरोप 63, दक्षिण पूर्व एशिया 278, पश्चिमी प्रशांत 139 कनाडा और ऑस्ट्रेलिया में, तपेदिक आदिवासी लोगों के बीच विशेष रूप से दूरदराज के क्षेत्रों में आम है। संयुक्त राज्य अमेरिका में आदिवासियों मे टीबी के कारण पांच गुनी अधिक मृत्यु दर है।\n",
            "22 मार्च 2020 की सुबह लगभग 6:24 बजे (सी.ई.टी) ज़ाग्रेब में 5.4 या 5.5 की तीव्रता पर भूकंप आया। इसका केन्द्र क्रोएशिया में 3 किलोमीटर (1.9 मील) दक्षिण-पश्चिम में और 7 किलोमीटर (4.3 मील) उत्तर में फैला था। मेर्साली तीव्रता परिमाप पर इसकी तीव्रता VII ( बहुत मजबूत) मापी गई। यह 1880 के भूकंप के बाद से ज़ाग्रेब का सबसे मजबूत भूकंप था।\n",
            "कॉपी कि, 38 वीं मंजिल!\n",
            "शानदार 80.\n",
            "78 रोमली एवेन्यू पश्चिम।\n",
            "यह एक डिटेक्टर है जिसे हमने प्रशिक्षित किया है 80 विभिन्न वर्गों पर माइक्रोसॉफ्ट के कोको डाटासेट में\n",
            "87 प्रतिशत खोई हुई विनिर्माण नौकरियों का सफाया इसलिए हुआ क्योंकि हमने सुधार किए हैं हमारी अपनी उत्पादकता में स्वचालन के माध्यम से।\n",
            "रियल काले बाजार पर '98 में बेच दिया गया था।\n",
            "यह सिर्फ आपको यह दिखाने के लिए है कि अगर आपके पास कैमरा है या विभिन्न तरह के सेंसर है क्युंकि यह ऊँचा है, इसकी ऊंचाई 1.8 मीटर है, तो अवरोधों को देख सकते हैं जैसे कि झाड़ीयां और उस तरह की चीज़े |\n",
            "इसकी शुरुवात 1908 मैं हुई, जब राईट बंधुओं ने पैरिस में उड़ान भरी, और सभी ने कहा, 'ओह, ये तो हम भी कर सकते हैं.' बहुत ही गिने-चुने लोग 1908 के शुरुवात में उड़ान भर सकते थे. अगले चार सालों में 39 देशों के पास सैंकड़ों हवाई जहाज़ थे, और हज़ारों विमान चालक. हवाई जहाज़ों का आविष्कार एक स्वाभाविक प्रक्रिया के तहत हुआ.\n",
            "'98 में हम '56 में विकसित की हुई चीज़ को वापस ले आए.\n",
            "फ़ॉन्ट रंग = \"# 80ff80 \" © @ पी आर एम nder एम @ nkÖÖ \n",
            "आप एक चाल करते हैं, इसे आप फिर 8O हैं की तरह है.\n",
            "वे 2008 से चला रहे हैं और पहले से ही हृदय के दौरे रोक दिए थे और अस्पताल के अंदर का संकट.\n",
            "1812 की लड़ाई में।\n",
            "वो महज़ 18 साल का था।\n",
            "मैं आज यहां एक कुत्ते के कारण मौजूद हूं : एक त्यक्त पिल्ला, जो मुझे सन् 1998 में बारिश में मिला।\n",
            "-लेफ्टिनेंट 82 था जब हम यहाँ मिल गया.\n",
            "टेरी. पहले आज, 62 के माध्यम से कैमरे 58 ऊपर लाओ.\n",
            "अब विकसित विश्व में यह 80 के आस-पास है |\n",
            "30 गुणा 800 बनता है ...\n",
            "80 साल पहले यह शहर हमले के अंतर्गत आता था।\n",
            "कृषि बेहतर की तुलना में 8 प्रतिशत में वृद्धि हुई।\n",
            "जेब में कागज़ रखना, 4: 08.\n",
            "लगभग 38 साल पहले, हम एक फ्लो साइटोमेट्री नामक तकनीक के साथ मेरी प्रयोगशाला में खेल रहे थे जो जैव चिकित्सा अनुसंधान के लिए विकसित किया गया था कैंसर कोशिकाओं जैसे कोशिकाओं का अध्ययन करने के लिए, लेकिन यह पता चला कि हम इसका इस्तेमाल कर रहे थे इस ऑफ-लेबल उद्देश्य के लिए जो फ़ायटोप्लान्कटन का अध्ययन करना था, और यह ऐसा करने के लिए अनुकूल था।\n",
            "चप्पू88तक बिक\n",
            "मैं उसकी उस बात का मतलब अभी समझ पा रहा हूँ 498 00: 38: 04,783\n",
            "तो आप देख रहे हैं, कि करारा, घाटी के साथ कि उच्च करारा , 2008 का वित्तीय संकट है .\n",
            "प्रस्ताव 8 के रूप में समलैंगिक विवाह उस समय मतदान के लिए तैयार था और जैसे-जैसे मतदान के परिणाम आने लगे उस रात यह साफ़ हो गया कि कैलिफ़ोर्निया के न्यायालयों ने जो हाल ही में समलैंगिक विवाहों की अनुमति दी थी वह वापस छीन ली जाने वाली थी।\n",
            "Endurance की घूमने की गति 67, 68 RPM है.\n",
            "शायद यह है 248\n",
            "डॉक्टर कविन्स्की, जो नैदानिक शोधकर्ता हैं कैलिफ़ोर्निया विश्विधालय में उन्होनें पाया की 70 की उम्र के एक में से तीन लोग और 85 की उम्र के आधे से ज्यादा लोग, अस्पताल में जाने से ज्यादा लाचार होकर वापस निकलते हैं।\n",
            "सीडीसी ने आने वाले 6 या 8 महीनों में... 5 मिलियन से लेकर 150 मिलियन... प्राणों की हानि का अनुमान लगाया है।\n",
            "इस फाइल में कोई .8 आदमी को आप के लिए इंतज़ार कर रही है।\n",
            "1982 पीटर गोल्लवितज़ेर(Peter Gollwitzer ) ने इस पर एक किताब लिखी, और 2009 में उन्होंने कुछ परीक्षण किये जो प्रकाशित हुए\n",
            "मवेशी कैम्प को 18 महीनों तक बढ़ाया गया।\n",
            "इसलिए -- (हंसता) 81 साल की उम्र में, मैं रिटायर नहीं हो सकता।\n",
            "जॉन, अध्याय 8, सुराह 12.\n",
            "अब उसी दिन की कल्पना करो -- जिसका तापमान 6, 8, या फिर 10 से १२ डिग्री अधिक हो उस दिन उस लू की लपट मे\n",
            "पी डब 381 काले टेम्पेस्ट, सिर्फ बॉक्स से बाहर . राउंड.\n",
            "किसी को 8 साल बाद जेल से रिहा कर रहा है ... ... तो क्या वह पढ़ना चाहिए ...\n",
            "बेवकूफ, सिर्फ 28 है.\n",
            "मेरी अपनी माँ, जो अब 82 वर्ष की हैं -- भगवान् उनका भला करे -- वो अस्पताल जाने से कतराती हैं क्यूंकि उन्हें इस सफर की योजना बनाने और प्रबंद करने में मुश्किल होती है\n",
            "दोहरी निकास, वी 8 एक '73 प्लायमाउथ 'Cuda, समझे.\n",
            "चेचन्या मे तो APT28 को पत्रकारोके ई मेल फोन हैक करना पसंदीदा काम था. जॉर्जिया की सरकार पूर्व युरोप रक्षा योजना लक्ष्य बना रशियन सरकारक जिसका इन्कार नही हो सकता.\n",
            "एक ही नेड स्टार्क 18 साल पहले रॉबर्ट को शपथ ली।\n",
            "यह 825 है.\n",
            "जी-8 शिखर सम्मेलन ने प्रस्ताव दिया है कि अफ़्रीका की समस्याओं का समाधान बड़ी मात्रा मे अनुदान देने में है, कुछ कुछ मार्शल प्लान जैसा ही.\n",
            "आप और अपने सामान यहाँ से बाहर नहीं कर रहे हैं अगर महोदय, 48 घंटे में, शेरिफ जबरन आप को दूर करने के लिए यहाँ होगा,\n",
            "(ठहाका) हमारे स्थानीय सरकार में 81 खन्ड हैं,\n",
            "2020 ग्रीष्मकालीन ओलम्पिक खेल, कोरोना वायरस की वजह से टोक्यो ओलंपिक का आयोजन एक साल के लिए टल गया है, आधिकारिक तौर पर XXXII ओलम्पियाड के खेल, एक अन्तराष्ट्रीय योजनाबद्ध बहु-खेल प्रतियोगिता है जिसका आयोजना 24 जुलाई से 9 अगस्त 2020 के बीच टोक्यो, जापान, में होना था। अब ओलंपिक खेल 23 जुलाई 2021 से 08 अगस्त 2021 में होगा। खेलों की मेजबानी के लिए तीन शहरों ने अपनी बोली पेश की थी: टोक्यो, मैड्रिड और इस्तांबुल। ओलम्पिक खेलों की संस्था अन्तर्राष्ट्रीय ओलम्पिक समिति ने 7 सितम्बर 2013 को ब्यूनस आयर्स, अर्जेंटीना, में अपने 125वें अधिवेशन में टोक्यो को मेजबान शहर घोषित किया था।\n",
            "उनका नया पुल 480 मीटर तक जाता था -- इससे पहले बने किसी भी प्रलम्बन पुल से 1.5 गुना ज़्यादा।\n",
            "हर रोज़ 805 मिलियन इंसान अपनी भूख मिटाने के लिए जूझते हैं, जिनमें से 30 मिलियन लोग यहाँ अमरीका में ही हैं।\n",
            "2008 से 2016 तक, तीस लाख से अधिक लोगों के \"हटाए जाने का आदेश हुआ\" - यह तकनीकी शब्द है निर्वासित किये जाने के लिए।\n",
            "73 से 83 तक:तीन प्रतिशत हर साल\n",
            "एक लिंकन एम के ज़ेड 37,000 डॉलर से लेकर 48,000 तक होगा।\n",
            "लगभग एकदम से अफ़्रीकी-अमेरिकियों को प्रस्ताव 8 की सफलता के लिए दोषी ठहराया जाने लगा।\n",
            "जी8 वाले भाषण के बाद से प्रेज़ीडेंट के ख़िलाफ़ धमकियां चार गुणा बढ़ गई हैं।\n",
            "कृ्षि में, जो 18 प्रतिशत ग़रीब नागरिकों की जीविका है, सिर्फ़ 18,000 करोड़ लगते हैं.\n",
            "जिस वायु कि हमें श्वास के लिए आदत है, ये उससे 8 गुना अधिक भारी है।\n",
            "मैनसन एक 28 वर्ष का इन्टीरियर डिज़ाइनर था, वह प्यारी सी एक बेटी और एक बेटे का पिता था, जिसने खुद को एक खण्डित न्याय प्रणाली के कारण जेल की सलाखों के पीछे पाया।\n",
            "0K8) '-\n",
            "इस धोखे धड़ी की कीमत इन कंपनियों के शेयरधारकों के लिए , और इसलिए समाज के लिए , प्रति वर्ष 380 अरब डॉलर है।\n",
            "लेकिन यू ट्यूब में हर मिनट 48 घंटे के विडियो अपलोड किये जाते हैं।\n",
            "लेकिन फ़र्क़ ये है, पृथ्वी पर, कार्बन समय के साथ वातावरण में से निकल कर, कोयला, तेल, प्राकृतिक गैस आदि के रूप में ज़मीन में जमा हो गया। शुक्र पर इसका अधिकांश भाग वातावरण में है। अन्तर यह है कि हमारा तापमान औसत 59 डिग्री सेल्सियस है। शुक्र पर यह 855 है। यह हमारी मौजूदा नीति के लिए प्रासंगिक है कि ज़्यादा कार्बन के जल्दी से जल्दी ज़मीन से बाहर ले जाना चाहिये और वातावरण में डाल देना चाहिये।\n",
            "48 घंटों में उसकी life हमेशा के लिए बदलने वाली है\n",
            "18 लाख.\n",
            "- किस समय विमान छूटता है? - 8:\n",
            "फ़ॉन्ट रंग = \"# 80ff80\" © @ पी आर एम!\n",
            "हम पहुंचेंगे 8: 20 पर, श्रीमान.\n",
            "अब, सुश्री कौओत और उनके सब पुस्तकालय सहयोगियों वास्तव में सबसे आगे रहे हैं कॉमिक्स की सिफारिश करने में , वास्तव में शुरुआती '80 के दशक से, जब एक स्कूल पुस्तकालय पत्रिका लेख कहा कि पुस्तकालय में केवल ग्राफिक उपन्यासों की उपस्थिति से उपयोग में लगभग 80 प्रतिशत की वृद्धि हुई और प्रसार में वृद्धि हुई गैर कॉमिक्स सामग्री के लगभग 30 प्रतिशत तक।\n",
            "और यह पता चला है कि अनुमान हैं कि सामूहिक 80,000 जीन है।\n",
            "आइए देखते हैं -- 2018 तक की प्रगति।\n",
            "... और बिस्तर में प्रेमी संग तेरी पत्नी को .38 की गोलियों से गूदी हुई पाती है.\n",
            "9.38 कृपया.\n",
            "रिपोर्ट के अनुसार 28 मार्च 2020 को लॉकडाउन के बाद दिल्ली की वायु गुणवत्ता सूचकांक में सुधार हुआ और वाहनों की आवाजाही कम हो गई।\n",
            "छह महीने और चार कानून फर्म के बाद में - (हँसी) जनवरी 2018 में, हमने पहली आईसीओ लॉन्च की जो लगभग 30 कंपनियों के मूल्य का प्रतिनिधित्व करता है और पूंजी जुटाने का एक बिल्कुल नया तरीका है।\n",
            "अरे, माउस, आपको लगता है कि 18 वर्षीय उद्देश्य से तैयार हैं?\n",
            "यदि 52 पत्तों का एक नया क्रमसंचय हर सेकंड लिखा जाता आज से 13.8 अरब वर्ष पहले शुरआत करके, जब ब्रह्माण्ड की उत्पत्ति हुई थी, तो आज भी हम लगातार लिख रहे होते और आने वाले लाखों वर्ष तक लिखते रहते।\n",
            "उन्के शब्दों और अर्थों पर ध्यान दे तो विजार्ड आफ ओज की तरह, या\"1984\" बिग ब्रदर की तरह, अपने मूल से आवाज को अलग करने से, कारण और प्रभाव को अलग करने से सर्वव्यापकता की भावना पैदा करता है ,\n",
            "टोंगा गर्त (21,582 फुट)\n",
            "यहां \"नेचर\" जर्नल से 2018 है 2017 से \"बायोसाइंस जर्नल,\" 2016 से नेशनल विज्ञान अकादमी।\n",
            "और इसलिए मेरी राय है हम इतना उत्सुक न हों, एकदम इनकी बातों में आने में, खासकर तब, जब बाहर का तापमान 48 डिग्री हो और आप पूरी बुर्का पहने हों |\n",
            "80,000 उपजातियों के पेड़ पौधों में ये लोग किस प्रकार रचना के आधार पर भिन्‍न दो असंबंधित पौधों की पहचान कर लेते हैं, जिन्‍हें जब मिलाया जाता है तो वे एक जैविक रसायन उत्‍पन्‍न करते हैं, जो कि जोड़े गए पदार्थों से कई गुणा अधिक प्रभावशाली होता है?\n",
            "Driver के खून में 2.8g शराब थी!\n",
            "मेरी 80 हजार डॉलर के अंदर था.\n",
            "यह 2008 की उस रात के बिलकुल विपरीत था जब प्रस्ताव 8 लागू हुआ था।\n",
            "तो पहले 24 घंटों के अंतराल में कभी-कभार निरीक्षण में चूक हो सकती है... प्रबंधन के 48 घंटों के उपरान्त, पर उसके बाद ठीक-ठाक काम करने लगेगा।\n",
            "यह कहानी 1980 और '90 के दशक में जारी है\n",
            "पिछले साल, हमारे सकल घरेलू उत्याद का 18 प्रतिशत तो स्वास्थ्य लागत में चला गया, पर लोगों को पता नहीं कि किस चीज़ की क्या कीमत है।\n",
            "अब हमें अवसर मिला है कि हम सभी सहभागियों को एक साथ ले आएं और दोबारा विचार करें कि 28 एकड़ भूमि को पार्क बनाने, उचित मूल्‍य के घर बनाने तथा स्‍थायी आर्थिक विकास के लिए किस प्रकार प्रयोग में लाएं।\n",
            "1867 की फरवरी में सरकार ने रॉबलिंग के प्रस्ताव को मंज़ूर कर दिया।\n",
            "15 मार्च, 1989 के इस दिन पर \" हम होगा\n",
            "1980 के दशक में रीगन क्रांति आई, जिससे अविनियम आया.\n",
            "मैं 8 इंच बढ़ी है और 60 पाउंड प्राप्त की.\n",
            "सुबह के 8: 45 बजे हैं।\n",
            "हम कह सकते हैं कि इन्हीं के कारण, उनके पूर्वजों के कारण, भारत ने पहला इंजीनियरिंग कॉलेज खोला 1847 में।\n",
            "और अब एक और अच्छी ख़बर है। यूएस का क्योटो में समर्थन करने वाले शहर अब 780 हैं\n",
            "- ये यहाँ पहली बार 48 साल पहले दीखाई दिया था .\n",
            "वह 28 वर्ष पुरानी है.\n",
            "न्यूनतम मजदूरी, 18 से 21 साल के बच्चों पर है?\n",
            "जब मैं लगभग 8 वर्ष की थी, तब मैंने पहली बार जलवायु परिवर्तन या ग्लोबल वार्मिंग के बारे में सुना था ।\n",
            "गबन 185 अच्छा लग रहा है.\n",
            "लेकिन अगर आप दूर से देखो, लेकिन अगर आप दूर से देखो, आप एक 825 मील की खाई को देखोगे न्यूयॉर्क सिटी और शिकागो के बीच जो पिछले कुछ वर्षों में बनाई गयी है एक कंपनी द्वारा - \"सपरेड नेटवर्क\".\n",
            "भैरवाकोना यहाँ से 80 मील दूर स्थित है.\n",
            "8।\n",
            "1870 से 1 9 70 तक, खेतों पर आधारित अमेरिकन श्रमिकों की 90 प्रतिशत गिरावट आई, और फिर 1 950 से 2010 तक, कारखानों में काम करते अमेरिकियों का प्रतिशत 75% तक गिर गया।\n",
            "लेकिन अब, बंगलादेश— में भी 80 के दशक में चमत्कार हो गया है। इमामों ने परिवार योजना को प्रोत्साहन देना शुरू कर दिया है। वे उस कोने से निकलकर आगे बढ़े और 90 में हम एचआईवी की भयंकर आपदा झेलते हैं\n",
            "अगले 80-90 वर्षों के लिए।\n",
            "ब्रैड मायर्स नामक इस स्नातक छात्र, 1985 में, फैसला किया कि वह इसका अध्ययन करेगा।\n",
            "यह सही है, क्योंकि याद करो, उन नौकरियों में से अधिकांश, 87 प्रतिशत, हमारी अपनी उत्पादकता में सुधार के कारण खो गए थे।\n",
            "लेकिन चीजों की पूरी विचार जैसे 1/2 + 1/4 + 1/8 + 1/16 और इत्यादि एक दृष्टिकोण है,यह उपयोगी है,अगर, आप हाथियों की एक लाइन खीँचना चाहते हो , प्रत्येक अगले एक की पूंछ पकदे है : सामान्य हाथी, युवा हाथी, हाथी के बच्चे, कुत्ते के आकार हाथी, पिल्ला आकार हाथी, सभी तरहश्री दाँत और पार\n",
            "18,000 साल.\n",
            "और क्या विडंबना है कि, तीस साल बाद आज हम 1-800-GOT-JUNK बना रहे हैं ?\n",
            "ब्रिटिश 1812 के युद्ध में व्हाइट हाउस को जला दिया बाद से नहीं... एक दुश्मन के बल पर कब्जा कर लिया अमेरिकी सत्ता का केंद्र है.\n",
            "2008 में, चक्रवात नरगिस ने म्यांमार को तबाह कर दिया.\n",
            "जो तुमने मुझे पहले से ही दिया है। जो तुम पूछ रहे हो कि मैं एक स्टेक डिनर पर 800 पाउंड खर्च करूँगा...\n",
            "बिल्कुल मेल खाती है. वो 33, 38 और 63 के थे जब ये तस्वीरें बनीं.\n",
            "8 अक्टूबर 2011 को, द लाइट इयर्स का अंत हो गया।\n",
            "...\"वह चंद घंटों में ही ... ..\"लोगों को दुनिया के एक कोने से से 24800: 42:\n",
            "3 September 1973 को शाम 6: 28 मीनट पर , एक नीले रंग की कीड़ा, जो अपने पंख एक मीनट में 70 बार फड़फड़ा सकता था ..\n",
            "28 आतिशदान\n",
            "जो लोग अपने 50 के दशक में अपने संबंधों से सबसे ज्यादा सन्तुष्ट थे वे 80 की उम्र मे सबसे स्वस्थ थे\n",
            "सभी इकाइयों, हम एक 2--36 है 840 शीतकालीन स्ट्रीट पर कार्य प्रगति पर।\n",
            "हम नवंबर 1918 को नहीं दोहराएंगे.\n",
            "उनको गिरफ़्तार किया गया क्योंकि उनसे और उनके परिवार से जुड़े $8 लाख का स्थानान्तरण कुछ निष्क्रिय खातों में चला गया ।\n",
            "अब एक अच्छी ख़बर है। 68 प्रतिशत अमेरिकी अब ये विश्वास करते हैं कि ग्लोबल वार्मिंग के लिये मानव गतिविधियाँ ही उत्तरदायी हैं। 69 प्रतिशत ये विश्वास करते हैं कि पृथ्वी उल्लेखनीय रुप से गर्म हो रही है। इसमें प्रगति हुई है, लेकिन कुन्जी ये है कि: जब भी हम उन चुनौतियों की सूची बनाते हैं, जिनका हमें सामना करना है, तो उसमें ग्लोबल वार्मिंग का स्थान लगभग सबसे अन्त में होता है।\n",
            "एक वर्ष में रवांडा 60000 और 80,000 के बीच रक्त की इकाइयां एकत्रित करता है।\n",
            "यह सब खत्म हो रहा है, मैं एक म्यूचुअल फंड में 18,000 खो दिया है, कॉलेज अपने बेटे के लिए पैसे.\n",
            "यह आकलन ओलंपिक खेलों में प्रयोग किये जाने वाले 18 तरण तालों के बराबर है।\n",
            "भेज रहे हैं 3स और 8स।\n",
            "एक और बात जानिए: एक औसत क़ैदी जो कैलिफ़ोरनिया के जेल सिस्टम में जाता है बिना किसी आर्थिक शिक्षा के, वो 30 सेंट प्रति घंटे कमाता है, क़रीब 800 डॉलर सालाना, उसकी कोई बचत वग़ैरह नहीं होती है,\n",
            "यह आप 18 से अधिक कर रहे हैं स्वीकार करते हैं कि एक मानक रिलीज फार्म है ... आप स्वेच्छा से सेक्स करने के लिए सहमति दे रहे हैं ... और उस बारे में हम प्रतिबद्ध करने के लिए कर रहे हैं कि कार्य करता है.\n",
            "इस अद्भुत दीक्षा के बाद, अचानक एक दिन उन्हें बाहर निकाला जाता है और 18 वर्ष की आयु के पश्चात वे अपने जीवन को पहली बार सूर्योदय के दर्शन करते हैं और इस जागरूकता के स्वच्छ क्षणों में सूर्य की पहली किरण हैरान करने वाले खूबसूरत प्राकृतिक नजारों में ढालानों पर अपनी छटा बिखेरती है, उन्होंने अब तक जो कुछ भी शिक्षा ग्रहण की होती है, वह अचानक ही उन्हें चौका देती है । अब पुरोहित पीछे हटकर कहता है \"आप देखिए ? यह बिल्कुल वैसा है जैसा मैंने आपको बताया था ।\n",
            "उनके 1 858 ओक पेड़ के सामने ...\n",
            "उनको पानी दीजिये, और वे 12 से 48 घंटोँ मेँ जी उठेंगे, हरे हो जायेंगे, बढना शुरू करेंगे|\n",
            "उसका 1980 में ब्रेमेन में निधन हो गया.\n",
            "यह 8: 30 है.\n",
            "और ग़ैरफ़ौजीकरण किया जोन से सभी 28,500 अमेरिकी सैनिकों वापस ले लें.\n",
            "और इसके लिए वो अपने जी8 पार्टनरों से मदद मांगेंगे।\n",
            "इसलिए प्रतिशोध में, वे ऐसा ही कर लगाते हैं अमेरिका से आयात किए जाने वाले सभी सामानों पर, और अदले बदले का यह छोटा सा खेल चलता है, और 20 प्रतिशत - बस कल्पना करो कि 20 प्रतिशत कर हर सामान, उत्पाद, उत्पाद घटक में जोड़ा जाता है सीमा पार आने जाने पर, और आप करों में 40 प्रतिशत की वृद्धि और भी देख सकते हैं, या 80 अरब डॉलर।\n",
            "आज, 1.8 अरब लोग युवा है 10 से 24 बीच दुनिया मे।\n",
            "मैने 26 घन्टे तक एक बन्द मगरमच्छ अन्ध मे, हवा मे 18 फ़ुट ऊपर फोटो ली।\n",
            "और एक शार्प 60 इंच एच डी टी वी की कीमत 898 डॉलर से 1,167 डॉलर तक होगी।\n",
            "चलिये साल 1986 में चलते हैं।\n",
            "बेल्जियम नवंबर 1918\n",
            "और मुझे भी यह नहीं पता था, जब तक मैंने कॉलेज में इन पाठ्यक्रमों को नहीं लिया, लेकिन तब उन दिनों उस समय - और यह 1980 के दशक की बात है - लोगों को इस बारे में बहुत नहीं पता था कि हमारी कोशिकाएं शर्करा लेपित क्यों हैं।\n",
            "विश्लेषण व्यक्तित्व आक्रामकता प्रतिबद्ध होने की संभावना 78.3% से पता चला...\n",
            "इम्मानुअल कांट से मिलो, 18 वीं सदी के जर्मन दार्शनिक सुपरस्टार।\n",
            "कहते है कि 8 ग्रह एक साथ इकट्ठे हो रहे हैं.\n",
            "जी-8 शिखर सम्मेलन.\n",
            "और शोधकर्ता आए और इन खेलों को 80 पूर्वस्कूली बच्चों को खिलाया।\n",
            "हम प्रत्येक भाग हैं ग्रह की जीवित प्रणाली का, जो लगभग 7.7 अरब मानवों के साथ बुना हुआ है और 180 लाख ज्ञात प्रजातियां।\n",
            "हमने उनकी पिछली 8 पुश्तों तक का अध्ययन किया और पाया कि केवल दो ही मृत्यु प्राकृतिक तौर पर हुई थी तथा जब हमने उन लोगों पर दबाव डालकर पूछताछ की, तो उन्होंने स्वीकार करते हुए बताया कि उनका एक साथी इतना बूढ़ा हो चुका था कि वह मरने की हालत में था, (हंसते हुए कहा) इसलिए हमने उसे मौत के घाट उतार दिया । परन्तु साथ ही उनके पास वन के बारे में चौका देने वाला सूक्ष्म ज्ञान था ।\n",
            "1988 में, आल्टर ने एऩएएनबीएच (NANBH) के नमूनों के पैनल में इसकी मौजूदगी को देखते हुए इस वायरस के होने की पुष्टि की.\n",
            "1872 में, इसने मुख्य इंजीनियर की लगभग जान ले ली।\n",
            "खैर, यह 97 है... . हम 98 जल्दी अप सवारी, वे आते हैं.\n",
            "सिर्फ इसे एक गणक में दर्ज करें, और यह आपको दर्शाएगा की कुल संभावित व्यवस्थाएं हैं 8.07 x 10^67, लगभग 8 के बाद 67 शून्य।\n",
            "158 दिन लगे एक नया बिजली कनेक्शन पाने के लिए\n",
            "अभी 6-8 महीने का समय लगेगा, पा हम समय को कम करने पर काम कर रहे हैं.\n",
            "तोहमेंशामिलहोने केइस शुक्रवार पर तीव्र टीवी पर 9/8 केन्द्रीय.\n",
            "इको माइक... 6-0-2-8... 5, सर.\n",
            "महबूबेह चाची, मेरी आपसे गुजारिश है की बच्चों का ख्याल रखियेगा 32801: 01: 20,200\n",
            "18वीं शताब्दी मे U.K. में जोन वेस्ले नामक बहुत महान धार्मिक उपदेशक हुआ जो पूरे देश मे घूम घूम कर लोगों को उपदेश देता और उन्हें जीने का सही तरीका बताता.\n",
            "ठीक है, देखो, 8: 00 पर इस पते सोमवार को हो सकता है.\n",
            "- ठीक है, फिर। 8 और अब\n",
            "उसका अचर्न में 27 अप्रैल, 1982 को निधन हो गया.\n",
            "और आखिर में ऐसा हुआ कि कप्तान डेविस और मैं... जानते हैं, यह एक दशक पहले की बात है, 2008 में... हम इतने सालों से एक-दूसरे के सम्पर्क में हैं।\n",
            "तो अब इस फैक्स को भेजने के लिए, मेरे पास पांच मिनट हैं\" या जो भी लोग 1985 में कर रहे थे।\n",
            "वह है अरनेस्ट बीदर्स की ली हुई फोटोग्राफ जो की 1968 में ली गई थी मेमफिस सैनीटेशन वरकर्स मार्च के दौरान जहाँ मर्द और औरतें साथ साथ खडे रहकर मानवता की पुष्टि कर रहे हैं।\n",
            "जब वह अपनी सूक के बाहर चलता है, दिल की दवा लेने एक फार्मेसी में जाता है जो रक्त को उसकी धमनियों में जमने से रोक सकती है , वह इस तथ्य का सामना करता है कि, बढ़ती महामारी के बावजूद वर्तमान में मिस्र में 82 प्रतिशत मौतें, यह दवाई इन्हे संबोधित कर सकती हैं कि नकली दवाएं , बुरी जीनियस हैं, निशाना लगाने का फैसला किया है।\n",
            "धारा 8 ने निश्चित किया\n",
            "मिशन, 185 बस चला ऑफ ग्रिड\n",
            "हंसी ! तो, कोई संदेह नहीं है, कुछ आकर्षक 1862 दिसंबर में हो रहा था जो हमारे बीच में अमेरिकियों को इसके बारे में पता होगा\n",
            "साइकिल समूह के 18 सदस्य गिरफ़्तार हुए.\n",
            "यह 1987 था।\n",
            "परिचर: £ 1,848.\n",
            "यहाँ गुरुत्वाकर्षण बहुत, बहुत ही मजेदार है जो की पृथ्वी का सिर्फ 80 प्रतिशत है.\n",
            "वर्ष 2078 में मैं अपना 75वां जन्मदिन मनाऊंगी ।\n",
            "हमारी टीम ने मानवता का ज्ञान छान मारा है समाधान के लिए ताकि ऊष्मा-जाल, जलवायु-परिवर्तन उत्सर्जन कम कर सके वातावरण में - \"किसी दिन, शायद,अगर हम भाग्यशाली हैं \"समाधान नही, 80 सर्वोत्तम प्रथाऐं और तकनीक पहले से ही हाथ में: स्वच्छ, नवीकरणीय ऊर्जा, सौर और पवन सहित; हरित इमारतों, दोनों नए और पुराना; कुशल परिवहन ब्राजील से चीन तक; संरक्षण और बहाली के माध्यम से संपन्न पारिस्थितिकी तंत्र; कचरे को कम करना और उसके मूल्य को पुनः प्राप्त करना; अच्छे तरीकों से भोजन बढ़ाना जो मिट्टी को पुन: उत्पन्न करता है; आहार को कम मांस, अधिक पौधों में स्थानांतरित करना; और महिलाओं और लड़कियों के लिए न्यायनीति।\n",
            "जब मैं नौ साल का था, उन्होंने यह किताब छपवाई, \"ब्लैक फोटोग्राफरस,1840-1940: जीवनी चलचित्र\"।\n",
            "LB: आप जानते हैं, टोनी फौसी इसमें हमारे गुरु हैं, और उन्होंने कहा कि 12 से 18 महीने।\n",
            "इसलिए यदि हम एक कंप्यूटर प्रति व्यक्ति देते हैं, और हम पाँच अरब का पाँच गुणा करें, भले ही वह लैपटॉप एक सौ डॉलर का हो, तो हमारे पास 483 हज़ार अरब डॉलर होंगे।\n",
            "स्क्वाड्रन 43 57 के माध्यम से, चतुर्थ भाग फ़ाक्सत्रोट 3889 के िलए स्विच!\n",
            "अबयह85 डिग्रीहै उदासखेलनेके लिएगिटार पर एक गीत के अधिकार के साथ.\n",
            "कॉपी49,ग्रिड22-185 काम\n",
            "यह शहर 800 साल पहले बनाया गया था।\n",
            "11 मार्च 2020 को, भारत के कैबिनेट सचिव, राजीव गौबा ने घोषणा की कि सभी राज्यों और केंद्र शासित प्रदेशों को महामारी रोग अधिनियम, 1897 की धारा 2 के प्रावधानों को लागू करना चाहिए।\n",
            "आपकी 08: 00 अपने रास्ते पर है.\n",
            "उदाहरण के लिए, यह हो सकता है शून्य होने की 70 प्रतिशत संभावना और एक होने का 30 प्रतिशत मौका या 80-20 या 60-40।\n",
            "78 रोमली एवेन्यू पूर्व?\n",
            "और 1980 में आखिरी ट्रेन चली\n",
            "हाँ, ई-स्पोर्ट्स है, पुरस्कार हैं, पैसा बनाने का अवसर है प्रतिस्पर्धात्मक तरीके से। 2: 48.56\n",
            "392\n",
            "473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDBMxbSfAWNA"
      },
      "source": [
        " \n",
        "      print(m)\n",
        "      k=k+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjkJ6HfKfE4e"
      },
      "source": [
        "TOKENIZING THE HINDI AND ENGLISH SENTENCES IN THE RESPECTIVE LISTS  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGoDRQGXfEa1"
      },
      "source": [
        "tok_hindi =  [[None]*400 for _ in range(len(hindi_list))] \n",
        "tok_english =  [[None]*400 for _ in range(len(english_list))] \n",
        "\n",
        "c=0\n",
        "for i in range(0,len(hindi_list)):\n",
        "  ht = tokenize_hindi(hindi_list[i].lower())\n",
        "  et = tokenize_english(english_list[i].lower())\n",
        "  if len(ht)>0 and len(et)>0 and len(ht)<=400 and len(et)<=400 and not (len(ht)==1 and ht[0]=='') and not (len(et)==1 and et[0]==''):\n",
        "    tok_hindi[c]=ht\n",
        "    tok_english[c]=et\n",
        "    c+=1\n",
        "    if c>100:\n",
        "      break\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fycsr7WrMQK0"
      },
      "source": [
        "FINAL TOKENIZED LIST ::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlpQuvGuDq6N"
      },
      "source": [
        "hindi_tokenized = [[] for _ in range(0,c)]\n",
        "english_tokenized = [[] for _ in range(0,c)]\n",
        "\n",
        "for el in range(0,c):\n",
        "\n",
        "  hindi_tokenized[el]=tok_hindi[el]\n",
        "  english_tokenized[el]=tok_english[el]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMI2up7J2RGg"
      },
      "source": [
        "VOCABULARY CLASS TO SET THE FREQUENCY COUNT , STRING TO INDEX MAPPING , INDEX TO STRING MAPPING AND VOCAB SIZE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGrA8uU7qXGA"
      },
      "source": [
        "class vocabulary:\n",
        "  def __init__(self,tokenized_list):\n",
        "    self.lists = tokenized_list\n",
        "    self.vocab_size=0\n",
        "    self.freq_count={}\n",
        "    self.stoi={}\n",
        "    self.itos={}\n",
        "  \n",
        "  # This function uses the tokenized_list and then creates the frequency count out of that\n",
        "  # by using a dictionary which will basically have a  \"word:count\" mapping\n",
        "  # hence store the count corresponding to the words\n",
        "  def get_freq_count(self):\n",
        "    # self pointer used for referring to the attributes of the class\n",
        "    for i in self.lists:\n",
        "      for j in range(0,len(i)):\n",
        "        if i[j] in self.freq_count:\n",
        "          self.freq_count[i[j]] = self.freq_count[i[j]]+1\n",
        "        else:\n",
        "          self.freq_count[i[j]] = 1\n",
        "\n",
        "    self.vocab_size = len(self.freq_count.keys())+4\n",
        "    # The vocab size 4 larger than the freq_count dictionary since we will also have\n",
        "    # unk , ss , es and pad in our vocab.\n",
        "    return self.freq_count,len(self.freq_count.keys())+4\n",
        "\n",
        "  #This function stores the string to index mapping for each of the words in the dictionary freq_count\n",
        "  def string_to_index(self):\n",
        "    st=0\n",
        "    self.stoi[\"<unk>\"]=0\n",
        "    self.stoi[\"<ss>\"]=1\n",
        "    self.stoi[\"<es>\"]=2\n",
        "    self.stoi[\"<pad>\"]=3\n",
        "    st=4\n",
        "    for k,v in self.freq_count.items():\n",
        "      self.stoi[k]=st\n",
        "      st+=1\n",
        "    #return self.stoi\n",
        "\n",
        "  #This function stores the index to string mapping for each of the words from the already set mapping of stoi\n",
        "  def index_to_string(self):\n",
        "    for k,v in self.stoi.items():\n",
        "      self.itos[v]=k\n",
        "\n",
        "    #return self.itos\n",
        "\n",
        "  # This function returns the vocab size \n",
        "  def vocab_size(self):\n",
        "\n",
        "    a,b = get_freq_count(self.lists)\n",
        "    self.vocab_size =b\n",
        "\n",
        "    #return b\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TH_Wrfn3ZqV"
      },
      "source": [
        "# Creating the object for the vocabulary class corresponding to the hindi language by passing\n",
        "# the tokenized list of hindi words \n",
        "h_obj = vocabulary(hindi_tokenized)\n",
        "h_obj.get_freq_count()\n",
        "h_obj.string_to_index()\n",
        "h_obj.index_to_string()\n",
        "\n",
        "# Similarly creating the object for the vocabulary class corresponding to the english language\n",
        "e_obj = vocabulary(english_tokenized)\n",
        "e_obj.get_freq_count()\n",
        "e_obj.string_to_index()\n",
        "e_obj.index_to_string()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTvEHFFHzir-"
      },
      "source": [
        "# Obtaining the string to index mapping for the hindi and english language \n",
        "h_stoi = h_obj.stoi\n",
        "h_itos = h_obj.itos\n",
        "e_stoi = e_obj.stoi\n",
        "e_itos = e_obj.itos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xqOx47gQUiM"
      },
      "source": [
        "CREATING BATCHES OF TOKENIZED SENTENCES ::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajyI-vHGRt3H"
      },
      "source": [
        "# Helper function used inside the create_batches() function which takes in a tokenized list and \n",
        "# return the same list just in place of a word we have the index corresponding to it\n",
        "def tokenizedlist_to_indexes(tokenized_list,stoi_dict):\n",
        "  final_indices = [[] for _ in range(0,c)]\n",
        "  for st in range(0,len(tokenized_list)):\n",
        "    llist = tokenized_list[st]\n",
        "    h1 = []\n",
        "    for j in range(0,len(llist)):\n",
        "      ele =  llist[j]\n",
        "      h1.append(stoi_dict[ele])\n",
        "    final_indices[st]=h1\n",
        "\n",
        "  return final_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUgWLeF-Zfl9"
      },
      "source": [
        "# Helper function that helps computing the max sentence length per batch so that it is easy\n",
        "# to make the sentences of similar length by padding and then combine them into same batch \n",
        "def max_length_per_batch(hindi_english_pair_list):\n",
        "  max_length_of_hindi_batch = []\n",
        "  max_length_of_english_batch = []\n",
        "  count=0\n",
        "  indx=0\n",
        "  maxa=0\n",
        "  maxb=0\n",
        "  a=0\n",
        "  b=0\n",
        "  for el in range(0,len(hindi_english_pair_list)):\n",
        "    count+=1\n",
        "    value = hindi_english_pair_list[el]\n",
        "    a = len(value[0])\n",
        "    b = len(value[1])\n",
        "    maxa = max(a,maxa)\n",
        "    maxb = max(b,maxb)\n",
        "    if count==batch_size:\n",
        "      max_length_of_hindi_batch.append(maxa)\n",
        "      max_length_of_english_batch.append(maxb)\n",
        "      maxa=0\n",
        "      maxb=0\n",
        "      a=0\n",
        "      b=0\n",
        "      count=0\n",
        "  \n",
        "  if maxa!=0:\n",
        "    max_length_of_hindi_batch.append(maxa)\n",
        "    max_length_of_english_batch.append(maxb)\n",
        "\n",
        "  return max_length_of_hindi_batch,max_length_of_english_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho4obDWy7UVa"
      },
      "source": [
        "# This function takes in the hindi and the english tokenized list along with the batch_size and h_stoi and e_stoi\n",
        "# This fn is mainly used for distributing the hindi and english sentences into batches of size =  batch_size\n",
        "# The batches created are such that they have all the sentences of similar length and if not , then of as\n",
        "# equal length as possible else we will apply the \"<pad>\" token to make all these of equal length\n",
        "\n",
        "def create_batches(hindi_tokenized,english_tokenized,batch_size,h_stoi,e_stoi):\n",
        "  \n",
        "  # This list will store the pairs of hindi and english sentence along with the length of hindi sentence which\n",
        "  # will be used as a criteria for dividing of sentences into batches.\n",
        "  hindi_english_pair_list = []\n",
        "\n",
        "  # CONVERT THE LIST OF TOKENS OF HINDI TO THE RESPECTIVE INDICES\n",
        "\n",
        "  hindi_indices = tokenizedlist_to_indexes(hindi_tokenized,h_stoi)\n",
        " \n",
        "  # CONVERT THE LIST OF TOKENS OF ENGLISH TO THE RESPECTIVE INDICES\n",
        "\n",
        "  english_indices = tokenizedlist_to_indexes(english_tokenized,e_stoi)\n",
        "\n",
        "  # hindi_english_pair_list will have the hindi_indices,english_indices and length of hindi_sentence\n",
        "  for el in range(0,len(hindi_indices)):\n",
        "    pairs = []\n",
        "    pairs.append(hindi_indices[el])\n",
        "    pairs.append(english_indices[el])\n",
        "    pairs.append(len(hindi_indices[el]))\n",
        "    hindi_english_pair_list.append(pairs)\n",
        "\n",
        "  # sort the list on the basis of the 3rd parameter ,i.e. the length of the hindi_sentence\n",
        "  # This is done so that now batches of similar length sentences can be grouped with each other\n",
        "  hindi_english_pair_list.sort(key=lambda x: x[2])\n",
        "\n",
        "  # computing max sentence length corresponding to each batch\n",
        "\n",
        "  max_length_of_hindi_batch, max_length_of_english_batch = max_length_per_batch(hindi_english_pair_list)\n",
        "\n",
        "  # Number of batches to be made each of size batch_size\n",
        "\n",
        "  no_of_batches = len(max_length_of_hindi_batch)\n",
        "\n",
        "  hindi_batches = [[] for _ in range(0,no_of_batches)]\n",
        "  english_batches = [[] for _ in range(0,no_of_batches)]\n",
        "\n",
        "  # last batch  which may be having lesser no of sentences than batch_size\n",
        "  last_batch_size = len(hindi_indices)%batch_size\n",
        "  \n",
        "\n",
        "  # Now , here for each of the values we will divide them into baches of size batch_size and length\n",
        "  # of each sentence will be max_length_of_english/hindi_batch + 2 since we will be adding a <ss> and\n",
        "  # <es> i.e. start of sentence and end of sentence token to each of them.\n",
        "  # For all those sentences of the batch , whose size is less than that of the largest sentence\n",
        "  # we append a  <pad> to it to make all sentences of one batch of equal size\n",
        "  count = 0\n",
        "  v=0\n",
        "  while v<len(hindi_english_pair_list):\n",
        "    m_len = max_length_of_hindi_batch[count]\n",
        "    single_batch = [[None]*(m_len+2) for _ in range(0,batch_size)]\n",
        "    for m in range(v,min(v+batch_size,len(hindi_english_pair_list))):\n",
        "      sent = hindi_english_pair_list[m][0]\n",
        "     # add padding  <pad>\n",
        "      while len(sent)<m_len :\n",
        "        sent.append(3)\n",
        "    # add <ss>\n",
        "      sent.insert(0,1)\n",
        "    # add <es>\n",
        "      sent.append(2)\n",
        "    # add this sentence to its batch\n",
        "      single_batch[m-v]=sent\n",
        "    \n",
        "    if count==len(max_length_of_hindi_batch)-1:\n",
        "      single_batch = single_batch[:last_batch_size]\n",
        "   \n",
        "    # converting each of the batches into a tensor and then converting it such that each column corresponds\n",
        "    # to one sentence mapping\n",
        "    single_batch_tensor =  torch.transpose(torch.tensor(single_batch),0,1)\n",
        "    \n",
        "    # The final hindi_batches list that contains the \n",
        "    hindi_batches[count]=single_batch_tensor\n",
        "    v=v+batch_size\n",
        "    count+=1\n",
        "\n",
        "# Repeat the same above thing for the english sentences i.e. to make english_batches\n",
        "  count = 0\n",
        "  v=0\n",
        "  while v<len(hindi_english_pair_list):\n",
        "    m_len = max_length_of_english_batch[count]\n",
        "    single_batch = [[None]*(m_len+2) for _ in range(0,batch_size)]\n",
        "    for m in range(v,min(v+batch_size,len(hindi_english_pair_list))):\n",
        "      sent = hindi_english_pair_list[m][1]\n",
        "      while len(sent)<m_len :\n",
        "        sent.append(3)\n",
        "      sent.insert(0,1)\n",
        "      sent.append(2)\n",
        "      single_batch[m-v]=sent\n",
        "    \n",
        "    if count==len(max_length_of_english_batch)-1:\n",
        "      single_batch = single_batch[:last_batch_size]\n",
        "\n",
        "    single_batch_tensor =  torch.transpose(torch.tensor(single_batch),0,1)\n",
        "    \n",
        "    english_batches[count]=single_batch_tensor\n",
        "    v=v+batch_size\n",
        "    count+=1\n",
        "\n",
        "  return hindi_batches,english_batches\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLDGuCFmZWey"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "l1, l2  = (create_batches(hindi_tokenized,english_tokenized,batch_size,h_stoi,e_stoi))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SbZOUAW0JfL"
      },
      "source": [
        "ENCODER CLASS WITH BI-GRU : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlKb2yEzRI-9"
      },
      "source": [
        "class Encode(nn.Module):\n",
        "  def __init__(self, e_input, e_embedding,\n",
        "                           e_hidden_size, num_layers):\n",
        "    super(Encode, self).__init__()\n",
        "\n",
        "    # Number of layers in the GRU\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # number of features for GRU to remember\n",
        "    self.hidden_size = e_hidden_size\n",
        "\n",
        "    # No of final nodes to drop in order to avoid overfitting\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    # applying a embedding on the given input_size i.e. the hindi vocabulary \n",
        "    self.embedding = nn.Embedding(e_input, e_embedding , padding_idx=3)\n",
        "    \n",
        "    #Applies a multi-layer GRU to an input sequence.\n",
        "\n",
        "    self.GRU = nn.GRU(e_embedding,e_hidden_size,2*num_layers, dropout = 0.5 , bidirectional=True)\n",
        "\n",
        "  # Dimensions of input [hindi_sentence_length, batch_size](Every sentence of the batch will have the same size since padding was used to make them of equal size)\n",
        "  def forward(self, input):\n",
        "    \n",
        "    # Dimensions of embedding[hindi_sentence_length  , batch_size , embedding_size]\n",
        "    # Here , the words of the sentence x for all the sentences of the batch size \n",
        "    # will be learning an embedding which was initially created using the constructor and function above\n",
        "    # i.e. nn.Embedding \n",
        "    embedding1 = self.embedding(input)\n",
        "                \n",
        "    # Avoid overfitting by dropping some of the nodes\n",
        "    embedding = self.dropout(embedding1)\n",
        "    \n",
        "   \n",
        "    # The embedding is then passed through the GRU layer\n",
        "    #Dimensions of output [hindi_sentence_length , batch_size , hidden_size*2]\n",
        "    #Dimensions of hidden state [num_layers*2, batch_size, hidden_size]\n",
        "    encoder_out , hidden_state = self.GRU(embedding)\n",
        "    #print(encoder_out.size())\n",
        "\n",
        "    hidden_final =torch.zeros(num_layers,input.shape[1],hidden_state.shape[2]*2).to(device)\n",
        "\n",
        "    # THIS IS DONE SINCE WHAT WE GET AS OUTPUT FROM A BI-GRU IS SOMETHING LIKE THIS ::\n",
        "    # [FORWARD1,BACKWARD1,FORWARD2,BACKWARD2............]\n",
        "    # SO  CONCATENATED THE OUTPUTS OF THE FORWARD AND BACKWARD OF EACH OF THE 2 CONSECUTIVE HIDDEN LAYERS \n",
        "    # AND THEN FED THE RESULTANT AS THE HIDDEN LAYER OUTPUT TO THE DECODER.\n",
        "    for i in range(0,(hidden_final.shape[0])):\n",
        "      hidden_final[i]=torch.cat((hidden_state[i*2,:,:],hidden_state[i*2+1,:,:]),dim=1)\n",
        "    \n",
        "\n",
        "    return encoder_out , hidden_final\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBe_c_n539aj"
      },
      "source": [
        "ATTENTION CLASS :: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4Rspxpb4BYr"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self,e_hidden_size,d_hidden_size):\n",
        "\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.attention  =  nn.Linear((2*e_hidden_size) + d_hidden_size , d_hidden_size)\n",
        "    self.enout = nn.Linear(2*e_hidden_size,d_hidden_size)\n",
        "    self.dhid  = nn.Linear(d_hidden_size,d_hidden_size)\n",
        "    self.del_d_hidden = nn.Linear(d_hidden_size, 1)\n",
        "\n",
        "  def forward(self,hidden,e_out,batch_size,hsentence_length):\n",
        "\n",
        "    # We need to repeat the hidden layer of decoder \"hsentence_length\" number of times\n",
        "    # This is because the e_out have the dimension of [hsentence_length,batch_size,hidden_dimension*2(bi)]\n",
        "    # and since when we are trying to find attention we are searching for all words of hsentence_length and \n",
        "    # the result we want after applying the operations, i.e. the final attention vector to be of size\n",
        "    # hsentence_length and hence , we need to repeat the hidden layer of decoder \"hsentence_length\"\n",
        "    # no of times.\n",
        "    # Initially-- #hidden = [num_of_layers,batch_size,d_hidden_size]\n",
        "        #e_out = [hsentence_length, batch_size, e_hidden_size]\n",
        "    #print(\"hidden1->\", hidden.size())\n",
        "    #print(\"eout1->\",e_out.size()) \n",
        "    '''\n",
        "    hidden1-> torch.Size([32, 1024])\n",
        "    eout1-> torch.Size([3, 32, 1024])\n",
        "    here1 -- torch.Size([32, 3, 1024]) torch.Size([32, 3, 1024])\n",
        "    here -- torch.Size([32, 3, 1024])\n",
        "    hidden2-> torch.Size([32, 3, 1024])\n",
        "    eout2-> torch.Size([32, 3, 1024])\n",
        "    '''\n",
        "\n",
        "    \n",
        "    hidden = hidden[:,None] \n",
        "\n",
        "    hidden = torch.repeat_interleave(hidden,repeats=hsentence_length,dim=1)\n",
        "        \n",
        "    #print(\"here1 --\", hidden.size() , hidden.squeeze(dim=0).size())\n",
        "\n",
        "\n",
        "    #hidden= hidden.squeeze(dim=0)\n",
        "\n",
        "    #print(\"here --\", hidden.size())\n",
        "    hidden = hidden.permute(0,1,2)\n",
        "\n",
        "    e_out = e_out.permute(1, 0, 2)\n",
        "    \n",
        "    # Now :: #hidden = [batch_size,hsentence_length,d_hidden_size]\n",
        "        #e_out = [batch_size,hsentence_length , e_hidden_size ]\n",
        "\n",
        "    # Now, applying the attention to the hidden of decoder and e_out by first concatenating them in the 3rd\n",
        "    # dimension and then applying an activation to it.\n",
        "    #print(\"hidden2->\", hidden.size())\n",
        "    #print(\"eout2->\",e_out.size())\n",
        "\n",
        "    concatenated_tensor = torch.cat((hidden, e_out), dim = 2)\n",
        "    \n",
        "    concatenated_tensor  = self.attention(concatenated_tensor)\n",
        "\n",
        "    #c2 = (self.enout(e_out)  + self.dhid(hidden))\n",
        "\n",
        "    #print(\"c2->\" ,c2.size())\n",
        "\n",
        "    #print(\"concatenated->\" , concatenated_tensor.size())\n",
        "    #print(concatenated_tensor.size())  #[32, 3, 1024]\n",
        "    attn_tensor = torch.tanh(concatenated_tensor)\n",
        "\n",
        "    #print(\"attn=>\" , attn_tensor.size())\n",
        "    #attn_tensor = [batch_size,hsentence_length, d_hidden_size]\n",
        "    \n",
        "    attention = self.del_d_hidden(attn_tensor) # [batch size, hsentence_length , 1] \n",
        "    \n",
        "    #print(\"attention=>\" , attention.size())\n",
        "\n",
        "    attention = attention.squeeze(dim=2)\n",
        "    #[batch size, hsentence_length]\n",
        "    #attention= [batch size,hsentence_length]\n",
        "\n",
        "    attention_resultant = F.softmax(attention, dim=1)\n",
        "    \n",
        "    #print(\"result->\" ,attention_resultant.size())\n",
        "\n",
        "\n",
        "    '''\n",
        "    attn=> torch.Size([32, 3, 1024])\n",
        "    attention=> torch.Size([32, 3, 1])\n",
        "    result-> torch.Size([32, 3])\n",
        "    '''\n",
        "    # This will give a attention vector wherein for every statement present in the batch \n",
        "    # the attention vector corresponding to it will contain what is the importance of each word\n",
        "    # of the sentence for the prediction of this new word so the softmax will convert the value corresponding\n",
        "    # to each word of the sentence in b/w 0 and 1 , such that the sum of importance of all the words\n",
        "    # of a sentence is equal to 1.\n",
        "\n",
        "    return attention_resultant\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykAwOy24tFuT"
      },
      "source": [
        "DECODER CLASS WITH GRU::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFLQuv8PRI78"
      },
      "source": [
        "class Decode(nn.Module):\n",
        "  def __init__(self, d_input, d_embedding,hidden_size, num_layers,result_size,attn,e_hidden_size):\n",
        "    \n",
        "    super(Decode, self).__init__()\n",
        "\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    # Output size of the word embedding NN\n",
        "    self.embedding_size = d_embedding\n",
        "\n",
        "    # Set the attention vector \n",
        "    self.attn = attn\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "\n",
        "    # The result which is obtained after the LSTM model results are passed through\n",
        "    # a fully connected layer to give the final vector which contains a probability\n",
        "    # corresponding to each word in the vocabulary.\n",
        "    #Set the final result size to be obtained \n",
        "    self.result_size = result_size\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    # Shape (hindi_vocab_size,embedding_size) [input size, embedding dims]\n",
        "    #The module that allows you to use Embedding is torch.nn.Embedding.\n",
        "    #It takes two parameters : the vocabulary size and the dimensionality of the embedding\n",
        "    self.embedding = nn.Embedding(d_input, d_embedding)\n",
        "\n",
        "    # Shape(embedding_size,no_of_layers_of_lstm,hidden_size) [embedding dims, hidden size, num layers]\n",
        "    self.GRU = nn.GRU(d_embedding + 2*e_hidden_size, hidden_size, num_layers, dropout = 0.5 , bidirectional=False)\n",
        "                             # This 1024 is for the 2*e_hidden_size\n",
        "\n",
        "\n",
        "    # Tried putting 1/4 to 1/2 of bias length=1\n",
        "    # in order to LSTM to remember more by default but that did not improve the results on the validation set.\n",
        "    # Similar to the earlier case of encoders\n",
        "    #m = self.e_LSTM._all_weights\n",
        "    #print(self.e_LSTM.bias_ih_l0.data[0:0])\n",
        "    #self.e_LSTM.bias_ih_l0.data[20:40]=1\n",
        "    #self.e_LSTM.bias_ih_l1.data[20:40]=1\n",
        "    #self.e_LSTM.bias_hh_l0.data[20:40]=1\n",
        "    #self.e_LSTM.bias_hh_l1.data[20:40]=1\n",
        "\n",
        "\n",
        "    # Shape(hidden_size,english_vocab_size) [embedding dims, hidden size, num layers]\n",
        "    #For fully connected layer, number of input features = number of hidden units in LSTM. \n",
        "    self.fully_connected = nn.Linear(2*e_hidden_size + hidden_size + d_embedding, result_size)\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "  # Shape of x (32) [batch_size]\n",
        "  def forward(self,output_from_prev, hidden_state , en_out ):\n",
        "\n",
        "    # Shape of x (1, 32) [1, batch_size]\n",
        "    output_from_prev = output_from_prev.unsqueeze(0)\n",
        "\n",
        "    hidden_state1 = hidden_state[-1]\n",
        "    # Shape(1, 32, embedding_Size) [1, batch_size, embedding dims]\n",
        "    embedding1 = self.dropout(self.embedding(output_from_prev))\n",
        "\n",
        "    #Applying the activation function to the embedding\n",
        "    embedding = F.relu(embedding1)\n",
        "    #print(\"passed hidden state-->\" ,hidden_state1.size())\n",
        "    # Passing the encoder outputs (i.e. the hidden states from the encoder) and the decoder's prev layer hidden\n",
        "    # unit to the attention function to get the value of the attention vector.\n",
        "    # from the attention function we get the value as [batch_size,en_out.shape[0]i.e. the length of the input hindi statement]\n",
        "    attention_value = self.attn(hidden_state1,en_out,embedding1.shape[1],en_out.shape[0])\n",
        "   #    attention_value = self.attn(hidden_state,en_out,'output_from_prev.shape[1]'32,en_out.shape[0])\n",
        "\n",
        "    # Now we need to use this attention to create a weighted vector , which will be having the importance \n",
        "    # corresponding to each word \n",
        "    # So on multiplying the attention_value with the en_out i.e. the hidden layers of the encoder we will\n",
        "    # get for each hidden layer the importance of every word for its contribution to predict the \n",
        "    # next word\n",
        "\n",
        "    # attention_value -> [batch_size,input_hindi_length]\n",
        "    # en_out -> [input_hindi_length, batch_size,e_hidden_size]\n",
        "    # now in order to get the weighted input whose dimension should be appropriate to be fed to the \n",
        "    # GRU layer i.e. it should be of the dimensions ->[1,batch_size,embedding_size+weighted_vector concatenated to it]\n",
        "    # so attention value from batch_size,input_hindi_length ----> [batch_size,1,input_hindi_length] and \n",
        "     \n",
        "    attention_value  = attention_value.unsqueeze(dim=1)\n",
        "\n",
        "    # en_out -> [batch_size , input_hindi_length ,e_hidden_size]\n",
        "\n",
        "    en_out  = en_out.permute(1,0,2)\n",
        "\n",
        "    # then multiplying both of them to give the weighted vector -> [batch_size,1,e_hidden_size]\n",
        "    \n",
        "    #weighted = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "    weighted_tensor = torch.matmul(attention_value,en_out)\n",
        "\n",
        "    # and then concatenating the embedding[1,batch_size,embedding_size] and the weighted vector [batch_size,1,e_hidden_size] (first permute to -> [1,batch_size,e_hidden_size])\n",
        "    weighted_tensor = weighted_tensor.permute(1,0,2)\n",
        "    #print(weighted_tensor.size())\n",
        "    #print(embedding.size())\n",
        "    concats = torch.cat((embedding,weighted_tensor),dim=2)\n",
        "   \n",
        "\n",
        "    # and then feeding as input to the GRU  \n",
        "    #print(\"concts->\" ,concats.size()) # 1,32,1324\n",
        "    #print(\"hiiiddd->\" ,hidden_state.size(), hidden_state.unsqueeze(0).size()) # 32,1024 and 1,32,1024\n",
        "  #################\n",
        "    results, hidden_state = self.GRU(concats,hidden_state)\n",
        "\n",
        "    # Shape of outputs (1, 32, hidden_size) [1, batch_size , hidden_size]\n",
        "    # Shape(no_of_layers_in_lstm, 32 ,hidden_size)[num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "   # results, hidden_state = self.GRU(embedding, hidden_state)\n",
        "\n",
        "    #results = results[:, :, :2*self.hidden_size] + results[:, :, 2*self.hidden_size:]\n",
        "\n",
        "    # Shape of predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "    #Before putting the lstm output into fc layer it has to be flattened out as done above using\n",
        "    # nn.Linear\n",
        "     #Output Size =  1*32*english_vocab_size -> as for each word in all the 64 batches simulatenously\n",
        "    # processing , there would be a prob associated with every word present in the vocab and the word with \n",
        "    # max prob is choosen as the value to be fed to the next LSTM input to 1st layer (this is done later on\n",
        "\n",
        "    # in the training step)\n",
        "    ##########################          prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "\n",
        "    english_pred = self.fully_connected(torch.cat((results,weighted_tensor,embedding),dim=2))\n",
        "    \n",
        "    \n",
        "    # Shape --> predictions [batch_size , output_size]\n",
        "    english_pred = english_pred.squeeze(0)\n",
        "    #print(\"early==>\",english_pred)\n",
        "    english_pred =  self.softmax(english_pred)\n",
        "    #print(\"last==>\",english_pred)\n",
        "    return english_pred, hidden_state\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTp52LYDuE0J"
      },
      "source": [
        "SETTING THE HYPERPARAMETERS FOR THE ENCODER AND DECODER::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciRHZQi4YLi_"
      },
      "source": [
        "batch_size=32\n",
        "input_size_encoder = h_obj.vocab_size\n",
        "encoder_embedding_size = 100\n",
        "e_hidden_size = 256\n",
        "num_layers = 2\n",
        "encoder_gru = Encode(input_size_encoder, encoder_embedding_size,\n",
        "                           e_hidden_size, num_layers).to(device)\n",
        "#print(encoder_gru)\n",
        "input_size_decoder = e_obj.vocab_size\n",
        "decoder_embedding_size = 100\n",
        "d_hidden_size = 512\n",
        "num_layers = 2\n",
        "output_size = e_obj.vocab_size\n",
        "teacher_forcing_ratio = 0.5\n",
        "attention  = Attention(e_hidden_size,d_hidden_size)\n",
        "decoder_gru = Decode(input_size_decoder, decoder_embedding_size,\n",
        "                           d_hidden_size, num_layers, output_size,attention,e_hidden_size).to(device)\n",
        "#print(decoder_gru)\n",
        "# THE SIZE OF THE INPUT TO THE DECODER and size of the result size would be same as the input to\n",
        "# the decoder is one word from each of the batch that is fed to the system of lstm simultaneously and the output\n",
        "# would be 1 word for all the 32 batches.\n",
        "learning_rate = 0.001\n",
        "\n",
        "# SGD Did not perform good for both the encoders and the decoders and Adam gave the best possible performance.\n",
        "#e_opt = optim.SGD(encoder_lstm.parameters(), lr=learning_rate)\n",
        "e_opt = optim.Adam(encoder_gru.parameters(), lr=learning_rate)\n",
        "#d_opt = optim.SGD(decoder_lstm.parameters(), lr=learning_rate)\n",
        "d_opt = optim.Adam(decoder_gru.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF8XjFoCABBL"
      },
      "source": [
        "OBTAINING THE BEST SET OF PREDICTIONS USING THE SEQ2SEQ MODEL (Helper function for the predictions function which predicts the english output for given hindi input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbTe7jVmIfpL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uP3bp2ytvol"
      },
      "source": [
        "def get_best_words(hind_list,largest_sentence,device):\n",
        "  eng_sentence = \"\"\n",
        "  input_sentence1 = torch.tensor(hind_list).resize_(len(hind_list),1).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    e_outs, hidden = encoder_gru.forward(input_sentence1)\n",
        "    \n",
        "    outputs = [1]\n",
        "   # We append a <sos> token to the start of the string and then get the output from the decoder lstm and then \n",
        "   # get the outputs in the output till we don't encounter a <eos> token.\n",
        "   # At the end we will get the index of the best prediction set of words.\n",
        "    for p in range(largest_sentence):\n",
        "      #last_output_word \n",
        "      last_out = outputs[-1]\n",
        "\n",
        "      with torch.no_grad():\n",
        "        #print(\"hhs->\" ,hidden.size())\n",
        "        output, hidden= decoder_gru.forward(torch.LongTensor([last_out]).to(device),hidden,e_outs)\n",
        "        bg1,bg2 = torch.max(output, 1)\n",
        "        #print(\"actual out\",bg1 , bg2)\n",
        "        highest_prob_word = bg2.item()\n",
        "        \n",
        "      if highest_prob_word!=2:\n",
        "        outputs.append(highest_prob_word)\n",
        "        eng_sentence += e_itos[highest_prob_word]\n",
        "        eng_sentence += \" \"\n",
        "      else:\n",
        "        break\n",
        "\n",
        "  return eng_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdBeB50PYLgP"
      },
      "source": [
        "def predictions(input_sentence, hindi, english, device, largest_sentence=500):\n",
        "     \n",
        "    hind_list  = tokenize_hindi(input_sentence)\n",
        "    hind_list.insert(0,\"<ss>\")\n",
        "    hind_list.append(\"<es>\")\n",
        "    t_indx = []\n",
        "    for word in hind_list:\n",
        "      if word in h_stoi:\n",
        "\t      t_indx.append(h_stoi[word])\n",
        "\n",
        "    # Tokenized the hindi sentence obtained from the dev set .. appended sos and eos token at the start and end of the token list\n",
        "    # and then converted the set of words in the tokens to the corresponding indices and hence we get the sentence to be fed as input\n",
        "    # to the encoder ...which on receiving the input , using the GRU return the hidden state , which when fed to the decoder ,gives the\n",
        "    # required output from which the maximum probability word is choosen.\n",
        "    # At the end the final english sentence is generated and returned.\n",
        "       \n",
        "    final_sentence = get_best_words(t_indx,largest_sentence,device)\n",
        "\n",
        "\n",
        "    if(final_sentence[-1]==' '):\n",
        "      final_sentence = final_sentence[:-1]\n",
        "    if(len(final_sentence)>1):\n",
        "      final_sentence = final_sentence[0].upper()+final_sentence[1:]\n",
        "    if(input_sentence[-1] in ['.','!','?']):\n",
        "      final_sentence = final_sentence + input_sentence[-1]\n",
        "      \n",
        "    return final_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsKhTlI7BJOy"
      },
      "source": [
        "BACK PROPAGATION ::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ep-TwSR3E--"
      },
      "source": [
        "def bptt(set_of_outs,english_sent):\n",
        "   # Clear the accumulating gradients\n",
        "    e_opt.zero_grad()\n",
        "    d_opt.zero_grad()\n",
        "    # Calculate the loss value for every epoch\n",
        "    # The loss function is such that it does not take into the set_of_outs and english_sent as it is \n",
        "    # but we need to convert their dimensionality\n",
        "\n",
        "    # This is done to remove the <sos> token i.e. the first token\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss = F.NLLLoss(set_of_outs[1:].reshape(-1,e_obj.vocab_size),\n",
        "                            english_sent[1:].reshape(-1), ignore_index=3)\n",
        "    \n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient value is it exceeds > 0.5 -> this is done to avoid the exploding gradient problem\n",
        "    # which sometimes starts giving NaN as the loss value\n",
        "    torch.nn.utils.clip_grad_norm_(encoder_gru.parameters(), max_norm=0.8)\n",
        "    torch.nn.utils.clip_grad_norm_(decoder_gru.parameters(), max_norm=0.8)\n",
        "    # Update the weights values using the gradients we calculated using backpropagation \n",
        "    e_opt.step()\n",
        "    d_opt.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozRt7TbyBP5R"
      },
      "source": [
        "TRAINING THE MODEL ::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LJSTxI1_dFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eed7288-f4c6-4c2d-b52b-2d91b2b41a91"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "# Training the model for 20 epochs , by taking batches from the training set iterator made and then\n",
        "# sending each of the sentences of the batch one by one as input to the encoder and then receiving\n",
        "# the results from decoder appending it to a set_of_outs array , comparing with the original sentence\n",
        "# and then backpropagating the loss.\n",
        "\n",
        "hindi_input= \"लानत है तुम पर!\"\n",
        "# Damn you!\n",
        "\n",
        "for q in range(0,10):\n",
        "  print(\"Epoch Number -> {} / {}\".format(q+1, 20))\n",
        "  #eng_output = predictions(hindi_input, l1, l2, device,largest_sentence=400)\n",
        "  eng_output = predictions(hindi_input, l1, l2, device,largest_sentence=400)\n",
        "  print(\"RESULT:: \" , eng_output)\n",
        "  break\n",
        "  #for every batch present \n",
        "  for index in range(0,len(l1)):\n",
        "    \n",
        "    # The hindi_sent and english_sent are the batches of sentences obtained from the respective hindi and english lists\n",
        "    hindi_sent = l1[index].to(device)\n",
        "    english_sent = l2[index].to(device)\n",
        "\n",
        "    # The hindi_sent is then passed through the encoder to get the hidden state which will then be \n",
        "    # forwarded to the decoder\n",
        "\n",
        "    en_outs , hidden_state = encoder_gru.forward(hindi_sent)\n",
        "    \n",
        "    # SOS TOKEN TO BE FIRST FED TO THE GRU \n",
        "    in_val = english_sent[0]\n",
        "    \n",
        "    # The final set_of_outs vecor which will be having the indexes of the words which will occur with maximum probability \n",
        "    set_of_outs = torch.zeros(english_sent.shape[0],english_sent.shape[1], e_obj.vocab_size).to(device)\n",
        "    \n",
        "    # The loop is iterated for the english_sentence length\n",
        "    for k in range(1,english_sent.shape[0]):\n",
        "\n",
        "        # The decoder provides the best predicted set of words in the output and also the hidden_state\n",
        "        # which is then passed to the next one as an input\n",
        "        output, hidden_state = decoder_gru.forward(in_val, hidden_state , en_outs)\n",
        "\n",
        "        set_of_outs[k] = output\n",
        "        \n",
        "        #0th dimension is batch size, 1st dimension is word embedding\n",
        "        #print(the word embedding having the highest probability with respect to the current word and hence\n",
        "        # get the index corresponding to that word)\n",
        "        bg2 = output.argmax(1) \n",
        "        # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "        if random.random() < teacher_forcing_ratio:\n",
        "          in_val = english_sent[k]\n",
        "        else : \n",
        "          in_val = bg2\n",
        "\n",
        "    # Perform the backpropagation to backpropagate the loss value and update the weights \n",
        "    loss_val = bptt(set_of_outs,english_sent)\n",
        "    #print(\"end=======================================================\")\n",
        "  print(\"Loss in each epoch -> {}\".format(loss_val))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Number -> 1 / 20\n",
            "RESULT::  When you new painting hill law applies admirable subtle unionseek phenomena carina another pequod fighting new <unk> global right decades will pequod basics phenomena whom at web rick black ticket thank pequod alive profession coming eddie fear on find when physically at phenomena regular pay will pequod meter <unk> scott what week pequod eyes too reasons gonna subtle british decks face program bandaged flu difference at gonna including profession at muslims pequod draw british more back fear worse when boundaries side pequod black alive alive meter truth yb completely costs week phenomena have evolves going regular you like black mean teleportation could d profession that going liked thoughts interviews push snare gonna yet information meter know stop new fighting its stolen feel law throughout alive fellow easier 111 alive push smaller decks easier a meter program know ideal rick you through alive pequod ideal that celled here thank physically power pay worse regular civil told too pequod alive a from washed another fear pequod basics warn people information halftime could computer back push consider including profession reasons you at is coin coming back me from draw pequod black alive bar worse side it ourselves sail dominant eyes at tried decks made decks <unk> back tell reasons accepted power things run moonlight differentiate at web world’s rahul scared at supporting means pequod thoughts reasons ticket years g ted our she ideal doug week which first gonna when know palestinians coin consider program know you the profession no profession yb prochlorococcus physically pequod ideal what understated web fear worse prisoner divided profession go than anti you about her back there physically at web family back fear phenomena pequod black boundaries back fear many ideal have halftime decks many long what alive profession yb gonna including ideal new pequod things run physically reef philosophers bandaged d can will fear epicenter pequod too pequod phenomena back washed both web decimated truth through percent at supporting admirable you foreigners protect you family divided fuck law microbes e that family when that law ideal extinct second heart best mirror web fine new pequod eyes meter truth eyes physically worse profession reasons disease its program eyes at accepted second things pequod alive new horn pequod feel me <unk> physically eyes you british pequod ideal stolen meter draw new ideal black hey both another another global week ourselves quiet your what experiment you learn ideal have goes web!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfsU6z8tBVqy"
      },
      "source": [
        "GETTING THE RESULTS ON DEV SET ::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ3_LYw9HKdG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "bc4ed122-806c-44ea-9f82-7346e5895371"
      },
      "source": [
        "f1=open(\"answer.txt\",\"a\")\n",
        "with open('hindistatements.csv', 'r') as file:\n",
        "  reader = csv.reader(file)\n",
        "  for row in reader:\n",
        "    hindi_word = row[2]\n",
        "    res1 = predictions(hindi_word, l1, l2, device,largest_sentence=500)\n",
        "    f1.write(res1)\n",
        "    f1.write(\"\\n\")\n",
        "f1.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-259909fa484a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"answer.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hindistatements.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhindi_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hindistatements.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7R5zgnxh3gm"
      },
      "source": [
        "import math\n",
        "def beam_search_decoder1(output_tensor, beam_width):\n",
        "  result_list = []\n",
        "  result_list.append([[], 0])\n",
        "  #print(type(result_list))\n",
        "  for ind in range(0,len(output_tensor)):\n",
        "    list_per_word = []\n",
        "\t\t# expand each current candidate\n",
        "    for i in range(0,len(result_list)):\n",
        "      seq= result_list[i][0]\n",
        "      prob_value = result_list[i][1]\n",
        "      for j in range(0, len(output_tensor[ind])):\n",
        "        l1 = result_list[i][0]\n",
        "        #print(type(output_tensor[ind][j]) , output_tensor[ind][j])\n",
        "        s1 = result_list[i][1] + (0-math.log(output_tensor[ind][j]))\n",
        "        ind_with_probval = (l1 + [j],s1)\n",
        "        list_per_word.append(ind_with_probval)\n",
        "      list_per_word.sort(key=lambda x:x[1])\n",
        "      list_per_word = list_per_word[0:beam_width]\n",
        "    result_list = list_per_word\n",
        "    \n",
        "  return result_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgSjN051hBfZ"
      },
      "source": [
        "def get_best_words_beam(hind_list,largest_sentence,device):\n",
        "  eng_sentence = \"\"\n",
        "  input_sentence1 = torch.tensor(hind_list).resize_(len(hind_list),1).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    e_outs, hidden = encoder_gru.forward(input_sentence1)\n",
        "    \n",
        "    outputs = [1]\n",
        "    prob_per_word = torch.empty(0,e_obj.vocab_size)\n",
        "   # We append a <sos> token to the start of the string and then get the output from the decoder lstm and then \n",
        "   # get the outputs in the output till we don't encounter a <eos> token.\n",
        "   # At the end we will get the index of the best prediction set of words.\n",
        "    for p in range(largest_sentence):\n",
        "      #last_output_word \n",
        "      last_out = outputs[-1]\n",
        "\n",
        "      with torch.no_grad():\n",
        "        #print(\"hhs->\" ,hidden.size())\n",
        "        output, hidden= decoder_gru.forward(torch.LongTensor([last_out]).to(device),hidden,e_outs)\n",
        "        bg1,bg2 = torch.max(output, 1)\n",
        "        highest_prob_word = bg2.item()\n",
        "        \n",
        "        \n",
        "      if highest_prob_word!=2:\n",
        "        outputs.append(highest_prob_word)\n",
        "        print(output)\n",
        "        prob_per_word = torch.cat((prob_per_word, output),0) \n",
        "        #eng_sentence += e_itos[highest_prob_word]\n",
        "        #eng_sentence += \" \"\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    print(prob_per_word.size())\n",
        "    print(len(outputs))\n",
        "\n",
        "    beam_search_decoder1(prob_per_word,3)\n",
        "  return eng_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srSR5kY8g5-Z"
      },
      "source": [
        "def predictions_beam_search(input_sentence, hindi, english, device, largest_sentence=500):\n",
        "     \n",
        "    hind_list  = tokenize_hindi(input_sentence)\n",
        "    hind_list.insert(0,\"<ss>\")\n",
        "    hind_list.append(\"<es>\")\n",
        "    t_indx = []\n",
        "    for word in hind_list:\n",
        "      if word in h_stoi:\n",
        "\t      t_indx.append(h_stoi[word])\n",
        "\n",
        "    # Tokenized the hindi sentence obtained from the dev set .. appended sos and eos token at the start and end of the token list\n",
        "    # and then converted the set of words in the tokens to the corresponding indices and hence we get the sentence to be fed as input\n",
        "    # to the encoder ...which on receiving the input , using the GRU return the hidden state , which when fed to the decoder ,gives the\n",
        "    # required output from which the maximum probability word is choosen.\n",
        "    # At the end the final english sentence is generated and returned.\n",
        "       \n",
        "    final_sentence = get_best_words_beam(t_indx,largest_sentence,device)\n",
        "    print(final_sentence)\n",
        "\n",
        "    if(final_sentence[-1]==' '):\n",
        "      final_sentence = final_sentence[:-1]\n",
        "    if(len(final_sentence)>1):\n",
        "      final_sentence = final_sentence[0].upper()+final_sentence[1:]\n",
        "    if(input_sentence[-1] in ['.','!','?']):\n",
        "      final_sentence = final_sentence + input_sentence[-1]\n",
        "      \n",
        "    return final_sentence"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}