{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phase-1(Seq2Seq).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBH9pFKKPKqu"
      },
      "source": [
        "!pip install torchtext==0.6.0 --quiet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from pprint import pprint\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "!pip install inltk\n",
        "from inltk.inltk import tokenize\n",
        "from torchtext import data\n",
        "import pandas as pd\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "\n",
        "!pip install Morfessor\n",
        "\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "\n",
        "! pip install --user indic\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "import re\n",
        "from indicnlp.tokenize import indic_detokenize  \n",
        "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsxTvzRAXdRM"
      },
      "source": [
        "# HINDI AND ENGLISH TOKENIZERS  :::::\n",
        "\n",
        "\n",
        "tok_hindi= []\n",
        "tok_english = []\n",
        "\n",
        "co=1\n",
        "\n",
        "def tokenize_hindi(text):\n",
        "  #global co\n",
        "  #co =co+1\n",
        "  #print(\"fd  \" , co)\n",
        "  #indic_string='लिए @ तरहQuiver♫एकज्या नाड़ीके♪ - [सिहरन]'\n",
        "  indic_string = text\n",
        "\n",
        "  res=\"\"\n",
        "#print('Input String: {}'.format(indic_string))\n",
        "#print('Tokens: ')\n",
        "  for text in indic_tokenize.trivial_tokenize(indic_string): \n",
        "   \n",
        "    #print(text)\n",
        "    text=text.replace('„', r'')\n",
        "    text=text.replace('“', r'')\n",
        "    text=text.replace('”', r'')\n",
        "    text=text.replace('–', r'')\n",
        "    text=text.replace('—', r'')\n",
        "    text=text.replace('´', r\"\")\n",
        "    text=text.replace('‘', r\"\")\n",
        "    text=text.replace('‚', r\"\")\n",
        "    text=text.replace('’', r\"\")\n",
        "    text=text.replace(\"''\", r'')\n",
        "    text=text.replace('´´', r'')\n",
        "    text=text.replace('…', r'')\n",
        "    text=text.replace('(',r'')\n",
        "    text=text.replace(')',r'')\n",
        "    text=text.replace('.',r'')\n",
        "    text=text.replace(',',r'')\n",
        "    text=text.replace('/',r'')\n",
        "    text=text.replace('[',r'')\n",
        "    text=text.replace(']',r'')\n",
        "    text=text.replace('♫',r'')\n",
        "    text=text.replace('♪',r'')\n",
        "    text=text.replace('-',r'')\n",
        "    text=text.replace('.',r'')\n",
        "    text=text.replace('।',r'')\n",
        "    text=text.replace('|',r'')\n",
        "    res=res+' '+text\n",
        "\n",
        "  reaesc = re.compile(r'[a-zA-Z]')\n",
        "  res = reaesc.sub('',res)\n",
        "  reaesc = re.compile(r'[@_♫♪!#$%^&*()?/|}{~:.]')\n",
        "  res = reaesc.sub('',res)\n",
        "   \n",
        "  return indic_tokenize.trivial_tokenize(res)\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_english(l2):\n",
        "  \n",
        "  l1 = []\n",
        "  #print(\"ec\")\n",
        "  reaesc = re.compile(r'[@_!♫♪#$%^&*(.,)<>?/\\|}{~:]')\n",
        "  text = reaesc.sub('',l2)\n",
        "  #print(text)\n",
        "  text=text.replace(\"-\",\"\")\n",
        "  text=text.replace(\"fuckin'\",\"fucking\")\n",
        "  text=text.replace(\"'ii\",\" will\")\n",
        "  text=text.replace(\"'ll\", r\" will\")\n",
        "  text=text.replace(\"can't\", r\"can not\")\n",
        "  text=text.replace(\"won't\", r\"will not\")\n",
        "  text=text.replace(\"n't\", r\" not\")\n",
        "  text=text.replace(\"'re\", r\" are\")\n",
        "  text=text.replace(\"i'm\", r\"i am\")\n",
        "  text=text.replace(\"'s\" , r\" is\")\n",
        "  text=text.replace(\"'ve\", r\" have\")\n",
        "  text=text.replace(\"'ve\" , r\"have\")\n",
        "  text=text.replace(\"cannot\" , r\"can not\")\n",
        "\n",
        "  m1 = \"\"\n",
        "  \n",
        "\n",
        "  doc = nlp(text)\n",
        " # for token in indic_tokenize.trivial_tokenize(l2):\n",
        "  \n",
        "  for token in doc:\n",
        "    reg = re.compile(r'[A-Za-z0-9]')\n",
        "    #print(token.text)\n",
        "    if reg.match(token.text):\n",
        "      l1.append(token.text)\n",
        "  \n",
        "  length = len(l1)\n",
        "\n",
        "  list1 = []\n",
        "  #print(l1)\n",
        "  for i in range(0,length-1):\n",
        "   # print(i,l1[i] , length)\n",
        "    if l1[i]=='ai' and l1[i+1]=='not':\n",
        "      l1[i] = \"ain't\"\n",
        "      i=i+1\n",
        "      list1.append(i)\n",
        "  \n",
        "  \n",
        "  #print(list1)\n",
        "  list2 = []\n",
        "\n",
        "  for i in range(0,len(l1)):\n",
        "     if i not in list1:\n",
        "       list2.append(l1[i])\n",
        "  return list2\n",
        "  \n",
        "  #return indic_tokenize.trivial_tokenize(l2)\n",
        "\n",
        "  #return (tokenize(res,'en'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB21n4FqQXTC"
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "#print(df)\n",
        "size = df.shape[0]\n",
        "l1 = 90000\n",
        "l2 = 10000\n",
        "l3 = size - l1 -l2\n",
        "\n",
        "#Removed the indexing from the csv file\n",
        "df = df.drop(df.columns[0], axis=1)\n",
        "\n",
        "# In this initially during training time , I divided the data into training and validation set , but in order to test\n",
        "# over the dev test trained it over all the present inputs \n",
        "df[0:size].to_csv(f\"tra_data.csv\", sep =',', index=False, index_label=False)\n",
        "df[100000:100020].to_csv(f\"val_data.csv\", sep =',', index=False, index_label=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tre-RZj9QXPA"
      },
      "source": [
        "# Firstly , fields are constructed , that will contain the information to process the data . It will take as parameter , the init_token (to be appended to\n",
        "# to the starting of the sentence) , the eos_token (to be appended to the end of the sentence) , (lower=True to convert the words to lowercase)\n",
        "# and the appropriate tokenizer to tokenize the input \n",
        "hindi = Field(sequential=True, use_vocab=True, init_token=\"<sos>\", eos_token=\"<eos>\" , lower=True, tokenize=tokenize_hindi)\n",
        "\n",
        "english = Field(sequential=True, use_vocab=True, init_token=\"<sos>\", eos_token=\"<eos>\" , lower=True,  tokenize=tokenize_english)\n",
        "\n",
        "\n",
        "# The fields to be constructed for the following columns - hindi and english\n",
        "fields = [\n",
        "  ('hindi', hindi),\n",
        "  ('english', english)\n",
        "]\n",
        "\n",
        "# loading the dataset into train_data(training data) and valid_data(validation data)\n",
        "train_data, valid_data = data.TabularDataset.splits(\n",
        "   path = '/content',\n",
        "   train = 'tra_data.csv',\n",
        "   validation = 'val_data.csv',\n",
        "   format = 'csv',\n",
        "   fields = fields,\n",
        "   skip_header = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDQffHWUQXKo"
      },
      "source": [
        "#Construct the Vocab object for this field from datasets \n",
        "# I Have taken the vocab size to be 50000 after experimenting with number of different values\n",
        "# Build vocab -> define a vocabulary object with attributes as frequency(store the frequency of tokens) , stoi(string to index mapping)\n",
        "# ans itos(index to string mapping)\n",
        "hindi.build_vocab(train_data, max_size=50000)\n",
        "english.build_vocab(train_data, max_size=500000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkJF0TpmQzvM"
      },
      "source": [
        "batch_size=32\n",
        "# Creates batches of data such that the sentences which are of similar lengths are batched together\n",
        "# in the same batch and they make the size of the sentence in each batch equal by adding padding\n",
        "# at the end of the sentences which are of lesser length than the largest available sentence\n",
        "train_iterator, valid_iterator= BucketIterator.splits((train_data, valid_data), \n",
        "                                                                      batch_size = batch_size, \n",
        "                                                                      sort_within_batch=True,\n",
        "                                                                      sort_key=lambda x: len(x.hindi),\n",
        "                                                                      device = device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlKb2yEzRI-9"
      },
      "source": [
        "class Encode(nn.Module):\n",
        "  def __init__(self, e_input, e_embedding,\n",
        "                           hidden_size, num_layers, dropout):\n",
        "    super(Encode, self).__init__()\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "\n",
        "    # number of features for LSTM to remember\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # No of final nodes to drop in order to avoid overfitting\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.tag = True\n",
        "\n",
        "    # applying a embedding on the given input_size. The input size here is len(hindi.vocab.stoi) which basically\n",
        "    # tells us that we need to make an embedding corresponding to every word present in our model so that at the \n",
        "    # time of testing , we can get the embedding corresponding to any word we want . The embedding basically represents each corresponding \n",
        "    # word into multiple dimensions.  \n",
        "\n",
        "    # We can use -->print(self.embedding.weight) to get the required embedding -> will also show requires_gradient = True which shows\n",
        "    # it is learnable i.e. it will learn the embeddings for the words that will be passed to it during the forward pass.\n",
        "    #     print((self.embedding.weight).size()) => will give input_size X embedding_size tensor\n",
        "\n",
        "    self.embedding = nn.Embedding(e_input, e_embedding)\n",
        "    \n",
        "    #Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n",
        "    self.LSTM = nn.LSTM(e_embedding, hidden_size, num_layers,dropout = dropout)\n",
        "\n",
        "    # Displays the weights and biases , choose the bias->0 as tried putting 1/4 to 1/2 of bias length=1\n",
        "    # in order to LSTM to remember more by default but that did not improve the results on the validation set.\n",
        "    #m = self.LSTM._all_weights\n",
        "    #print(self.LSTM.bias_ih_l0.data[0:0])\n",
        "    #self.LSTM.bias_ih_l0.data[20:40]=1\n",
        "    #self.LSTM.bias_ih_l1.data[20:40]=1\n",
        "    #self.LSTM.bias_hh_l0.data[20:40]=1\n",
        "    #self.LSTM.bias_hh_l1.data[20:40]=1\n",
        "\n",
        "  # Dimensions of x [Sentence_length, batch_size](Every sentence of the batch will have the same size since padding was used to make them of equal size)\n",
        "  def forward(self, x):\n",
        "\n",
        "    \n",
        "    # Dimensions of embedding[Sequence_length , batch_size , embedding dims]\n",
        "    # Here , the words of the sentence x for all the sentences of the batch size (32)\n",
        "    # will be learning an embedding which was initially created using the constructor and function above\n",
        "    # i.e. nn.Embedding \n",
        "    embedding1 = self.embedding(x)\n",
        "                \n",
        "    # Avoid overfitting by dropping some of the nodes\n",
        "    embedding = self.dropout(embedding1)\n",
        "    \n",
        "    #Dimensions of output [Sequence_length , batch_size , hidden_size]\n",
        "    #Dimensions of hidden state and cell state [num_layers, batch_size size, hidden_size]\n",
        "    # The embedding is then passed through the LSTM layer which uses the hidden state and cell state + previously obtained outputs\n",
        "    encoder_out , (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "    return hidden_state, cell_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFLQuv8PRI78"
      },
      "source": [
        "class Decode(nn.Module):\n",
        "  def __init__(self, d_input, d_embedding,hidden_size, num_layers, drop, result_size):\n",
        "    \n",
        "    super(Decode, self).__init__()\n",
        "\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "    # Output size of the word embedding NN\n",
        "    self.embedding_size = d_embedding\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "    # The result which is obtained after the LSTM model results are passed through\n",
        "    # a fully connected layer to give the final vector which contains a probability\n",
        "    # corresponding to each word in the vocabulary.\n",
        "    self.result_size = result_size\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(drop)\n",
        "\n",
        "    # Shape (hindi_vocab_size,embedding_size) [input size, embedding dims]\n",
        "    #The module that allows you to use Embedding is torch.nn.Embedding.\n",
        "    #It takes two parameters : the vocabulary size and the dimensionality of the embedding\n",
        "    self.embedding = nn.Embedding(d_input, d_embedding)\n",
        "\n",
        "    # Shape(embedding_size,no_of_layers_of_lstm,hidden_size) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(d_embedding, hidden_size, num_layers, dropout = drop)\n",
        "\n",
        "    # Tried putting 1/4 to 1/2 of bias length=1\n",
        "    # in order to LSTM to remember more by default but that did not improve the results on the validation set.\n",
        "    # Similar to the earlier case of encoders\n",
        "    #m = self.e_LSTM._all_weights\n",
        "    #print(self.e_LSTM.bias_ih_l0.data[0:0])\n",
        "    #self.e_LSTM.bias_ih_l0.data[20:40]=1\n",
        "    #self.e_LSTM.bias_ih_l1.data[20:40]=1\n",
        "    #self.e_LSTM.bias_hh_l0.data[20:40]=1\n",
        "    #self.e_LSTM.bias_hh_l1.data[20:40]=1\n",
        "\n",
        "\n",
        "    # Shape(hidden_size,english_vocab_size) [embedding dims, hidden size, num layers]\n",
        "    #For fully connected layer, number of input features = number of hidden units in LSTM. \n",
        "    self.fc = nn.Linear(hidden_size, result_size)\n",
        "\n",
        "  # Shape of x (32) [batch_size]\n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "\n",
        "    # Shape of x (1, 32) [1, batch_size]\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    # Shape(1, 32, embedding_Size) [1, batch_size, embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # Shape of outputs (1, 32, hidden_size) [1, batch_size , hidden_size]\n",
        "    # Shape(no_of_layers_in_lstm, 32 ,hidden_size)[num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "    results, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "\n",
        "     \n",
        "    # Shape of predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "    #Before putting the lstm output into fc layer it has to be flattened out as done above using\n",
        "    # nn.Linear\n",
        "     #Output Size =  1*32*english_vocab_size -> as for each word in all the 64 batches simulatenously\n",
        "    # processing , there would be a prob associated with every word present in the vocab and the word with \n",
        "    # max prob is choosen as the value to be fed to the next LSTM input to 1st layer (this is done later on\n",
        "    # in the training step)\n",
        "    english_pred = self.fc(results)\n",
        "\n",
        "    # Shape --> predictions [batch_size , output_size]\n",
        "    english_pred = english_pred.squeeze(0)\n",
        "\n",
        "    return english_pred, hidden_state, cell_state\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciRHZQi4YLi_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a81416d-b86b-4b44-fb79-abadead03fbd"
      },
      "source": [
        "batch_size=32\n",
        "input_size_encoder = len(hindi.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "encoder_lstm = Encode(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "print(encoder_lstm)\n",
        "input_size_decoder = len(english.vocab)\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = 0.5\n",
        "output_size = len(english.vocab)\n",
        "\n",
        "decoder_lstm = Decode(input_size_decoder, decoder_embedding_size,\n",
        "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
        "print(decoder_lstm)\n",
        "# THE SIZE OF THE INPUT TO THE DECODER and size of the result size would be same as the input to\n",
        "# the decoder is one word from each of the batch that is fed to the system of lstm simultaneously and the output\n",
        "# would be 1 word for all the 32 batches.\n",
        "learning_rate = 0.001\n",
        "\n",
        "# SGD Did not perform good for both the encoders and the decoders and Adam gave the best possible performance.\n",
        "#e_opt = optim.SGD(encoder_lstm.parameters(), lr=learning_rate)\n",
        "e_opt = optim.Adam(encoder_lstm.parameters(), lr=learning_rate)\n",
        "#d_opt = optim.SGD(decoder_lstm.parameters(), lr=learning_rate)\n",
        "d_opt = optim.Adam(decoder_lstm.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encode(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(41993, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            ")\n",
            "Decode(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(32814, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=1024, out_features=32814, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uP3bp2ytvol"
      },
      "source": [
        "def get_best_words(input_sentence,largest_sentence,device):\n",
        "  with torch.no_grad():\n",
        "    hidden, cell = encoder_lstm.forward(input_sentence)\n",
        "    \n",
        "    outputs = [2]\n",
        "   # We append a <sos> token to the start of the string and then get the output from the decoder lstm and then \n",
        "   # get the outputs in the output till we don't encounter a <eos> token.\n",
        "   # At the end we will get the index of the best prediction set of words.\n",
        "    for p in range(largest_sentence):\n",
        "      last_output_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        output, hidden, cell = decoder_lstm.forward(last_output_word, hidden, cell)\n",
        "        bg1,bg2 = torch.max(output, 1)\n",
        "        highest_prob_word = bg2.item()\n",
        "        \n",
        "      outputs.append(highest_prob_word)\n",
        "\n",
        "      if highest_prob_word == 3:\n",
        "        break\n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdBeB50PYLgP"
      },
      "source": [
        "def predictions(sentence, hindi, english, device, largest_sentence=500):\n",
        "     \n",
        "    reaesc = re.compile(r'[a-zA-Z]')\n",
        "    res = reaesc.sub(' ',sentence)\n",
        "    reaesc = re.compile(r'[@_!#$%^&*()<>?/|}{~:]')\n",
        "    res = reaesc.sub(' ',res)\n",
        "    hind_list = indic_tokenize.trivial_tokenize(res)\n",
        "    hind_list.insert(0, hindi.init_token)\n",
        "    hind_list.append(hindi.eos_token)\n",
        "    # Tokenized the hindi sentence obtained from the dev set .. appended sos and eos token at the start and end of the token list\n",
        "    # and then converted the set of words in the tokens to the corresponding indices and hence we get the sentence to be fed as input\n",
        "    # to the encoder ...which on receiving the input , using the LSTMS return the hidden and cell state , which when fed to the decoder ,gives the\n",
        "    # required output from which the maximum probability word is choosen.\n",
        "    # At the end the final english sentence is generated and returned.\n",
        "    token_index=[]\n",
        "\n",
        "    for word in hind_list:\n",
        "\t    token_index.append(hindi.vocab.stoi[word])\n",
        "    \n",
        "    input_sentence = torch.tensor(token_index).unsqueeze(1).to(device)\n",
        "   \n",
        "    outputs = get_best_words(input_sentence,largest_sentence,device)\n",
        "  \n",
        "    translated_sentence=[]\n",
        "    # Translate the obtain indices to the required words\n",
        "    for val in outputs:\n",
        "\t\t    translated_sentence.append(english.vocab.itos[val])\n",
        "    \n",
        "    if(\"<eos>\" in translated_sentence):\n",
        "      translated_sentence.remove(\"<eos>\")\n",
        "    if(\"<sos>\" in translated_sentence):\n",
        "      translated_sentence.remove(\"<sos>\")\n",
        "\n",
        "    ans = ' '.join(translated_sentence)\n",
        "    if(len(ans)>1):\n",
        "      ans = ans[0].upper()+ans[1:]\n",
        "    if(sentence[-1] in ['.','!','?']):\n",
        "      ans = ans + sentence[-1]\n",
        "    return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LJSTxI1_dFu"
      },
      "source": [
        "# Training the model for 20 epochs , by taking batches from the training set iterator made and then\n",
        "# sending each of the sentences of the batch one by one as input to the encoder and then receiving\n",
        "# the results from decoder appending it to a set_of_outs array , comparing with the original sentence\n",
        "# and then backpropagating the loss.\n",
        "for q in range(0,20):\n",
        "  print(\"Epoch Number -> {} / {}\".format(q+1, 20))\n",
        "  for batch in train_iterator:\n",
        "\n",
        "    hindi_sent = batch.hindi.to(device)\n",
        "    english_sent = batch.english.to(device)\n",
        "\n",
        "    # The final set_of_outs vecor which will be having the indexes of the words which will occur with maximum probability \n",
        "    set_of_outs = torch.zeros(english_sent.shape[0],hindi_sent.shape[1], len(english.vocab)).to(device)\n",
        "    \n",
        "    hidden_state, cell_state = encoder_lstm.forward(hindi_sent)\n",
        "    \n",
        "    # SOS TOKEN TO BE FIRST FED TO THE LSTM \n",
        "    in_val = english_sent[0]\n",
        "    \n",
        "    # The loop is iterated for all the \n",
        "    for k in range(1,english_sent.shape[0]):\n",
        "   \n",
        "        output, hidden_state, cell_state = decoder_lstm.forward(in_val, hidden_state, cell_state)\n",
        "        \n",
        "        set_of_outs[k] = output\n",
        "        \n",
        "        #0th dimension is batch size, 1st dimension is word embedding\n",
        "        #print(the word embedding having the highest probability with respect to the current word and hence\n",
        "        # get the index corresponding to that word)\n",
        "        bg2 = output.argmax(1) \n",
        "        # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "        in_val = english_sent[k] if random.random() < 0.5 else bg2 \n",
        "      \n",
        "    # Clear the accumulating gradients\n",
        "    e_opt.zero_grad()\n",
        "    d_opt.zero_grad()\n",
        "    # Calculate the loss value for every epoch\n",
        "    # The loss function is such that it does not take into the set_of_outs and english_sent as it is \n",
        "    # but we need to convert their dimensionality\n",
        "    set_of_outs = set_of_outs.view(-1, len(english.vocab))\n",
        "    # This is done to remove the <sos> token i.e. the first token\n",
        "    set_of_outs = set_of_outs[1:]\n",
        "    english_sent= english_sent.view(-1)   \n",
        "    english_sent= english_sent[1:]\n",
        "    loss = criterion(set_of_outs,english_sent)\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss.backward()\n",
        "    # Clip the gradient value is it exceeds > 1 - this is done to avoid the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(encoder_lstm.parameters(), max_norm=1)\n",
        "    torch.nn.utils.clip_grad_norm_(decoder_lstm.parameters(), max_norm=1)\n",
        "    # Update the weights values using the gradients we calculated using backpropagation \n",
        "    e_opt.step()\n",
        "    d_opt.step()\n",
        "    #print(\"end=======================================================\")\n",
        "  print(\"Loss in each epoch -> {}\".format(loss.item()))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ3_LYw9HKdG"
      },
      "source": [
        "df = pd.read_csv(\"hindistatements.csv\")\n",
        "\n",
        "df = df[\"hindi\"]\n",
        "\n",
        "\n",
        "f1=open(\"answer.txt\",\"a\")\n",
        "for i in range(0,4999):\n",
        "  src = df.iloc[i]\n",
        "  res1 = predictions(src, hindi, english, device,largest_sentence=500)\n",
        "  f1.write(res1)\n",
        "  f1.write(\"\\n\")\n",
        " \n",
        "src = df.iloc[4999]\n",
        "f1.write(predictions(src, hindi, english, device,largest_sentence=500))\n",
        "\n",
        "f1.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}